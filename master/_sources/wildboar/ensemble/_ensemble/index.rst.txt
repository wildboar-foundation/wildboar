:orphan:

:py:mod:`wildboar.ensemble._ensemble`
=====================================

.. py:module:: wildboar.ensemble._ensemble


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   wildboar.ensemble._ensemble.BaggingClassifier
   wildboar.ensemble._ensemble.BaggingRegressor
   wildboar.ensemble._ensemble.BaseBagging
   wildboar.ensemble._ensemble.BaseForestClassifier
   wildboar.ensemble._ensemble.BaseForestRegressor
   wildboar.ensemble._ensemble.BaseShapeletForestClassifier
   wildboar.ensemble._ensemble.BaseShapeletForestRegressor
   wildboar.ensemble._ensemble.ExtraShapeletTreesClassifier
   wildboar.ensemble._ensemble.ExtraShapeletTreesRegressor
   wildboar.ensemble._ensemble.ForestMixin
   wildboar.ensemble._ensemble.IntervalForestClassifier
   wildboar.ensemble._ensemble.IntervalForestRegressor
   wildboar.ensemble._ensemble.IsolationShapeletForest
   wildboar.ensemble._ensemble.PivotForestClassifier
   wildboar.ensemble._ensemble.ProximityForestClassifier
   wildboar.ensemble._ensemble.RockestClassifier
   wildboar.ensemble._ensemble.RockestRegressor
   wildboar.ensemble._ensemble.ShapeletForestClassifier
   wildboar.ensemble._ensemble.ShapeletForestEmbedding
   wildboar.ensemble._ensemble.ShapeletForestRegressor




.. py:class:: BaggingClassifier(base_estimator=None, n_estimators=10, *, max_samples=1.0, bootstrap=True, oob_score=False, class_weight=None, warm_start=False, n_jobs=None, random_state=None, verbose=0)

   Bases: :py:obj:`BaseBagging`, :py:obj:`sklearn.ensemble.BaggingClassifier`

   Base class for all estimators in scikit-learn.

   .. rubric:: Notes

   All estimators should specify all the parameters that can be set
   at the class level in their ``__init__`` as explicit keyword
   arguments (no ``*args`` or ``**kwargs``).

   .. py:method:: fit(x, y, sample_weight=None)

      Build a Bagging ensemble of estimators from the training set (X, y).

      :param X: The training input samples. Sparse matrices are accepted only if
                they are supported by the base estimator.
      :type X: {array-like, sparse matrix} of shape (n_samples, n_features)
      :param y: The target values (class labels in classification, real numbers in
                regression).
      :type y: array-like of shape (n_samples,)
      :param sample_weight: Sample weights. If None, then samples are equally weighted.
                            Note that this is supported only if the base estimator supports
                            sample weighting.
      :type sample_weight: array-like of shape (n_samples,), default=None

      :returns: **self** -- Fitted estimator.
      :rtype: object



.. py:class:: BaggingRegressor(base_estimator=None, n_estimators=100, *, max_samples=1.0, bootstrap=True, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)

   Bases: :py:obj:`BaseBagging`, :py:obj:`sklearn.ensemble.BaggingRegressor`

   Base class for all estimators in scikit-learn.

   .. rubric:: Notes

   All estimators should specify all the parameters that can be set
   at the class level in their ``__init__`` as explicit keyword
   arguments (no ``*args`` or ``**kwargs``).

   .. py:method:: fit(x, y, sample_weight=None)

      Build a Bagging ensemble of estimators from the training set (X, y).

      :param X: The training input samples. Sparse matrices are accepted only if
                they are supported by the base estimator.
      :type X: {array-like, sparse matrix} of shape (n_samples, n_features)
      :param y: The target values (class labels in classification, real numbers in
                regression).
      :type y: array-like of shape (n_samples,)
      :param sample_weight: Sample weights. If None, then samples are equally weighted.
                            Note that this is supported only if the base estimator supports
                            sample weighting.
      :type sample_weight: array-like of shape (n_samples,), default=None

      :returns: **self** -- Fitted estimator.
      :rtype: object



.. py:class:: BaseBagging(base_estimator=None, n_estimators=10, *, max_samples=1.0, bootstrap=True, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)

   Bases: :py:obj:`wildboar.base.BaseEstimator`, :py:obj:`sklearn.ensemble._bagging.BaseBagging`

   Base class for all estimators in scikit-learn.

   .. rubric:: Notes

   All estimators should specify all the parameters that can be set
   at the class level in their ``__init__`` as explicit keyword
   arguments (no ``*args`` or ``**kwargs``).

   .. py:method:: fit(x, y, sample_weight=None)

      Build a Bagging ensemble of estimators from the training set (X, y).

      :param X: The training input samples. Sparse matrices are accepted only if
                they are supported by the base estimator.
      :type X: {array-like, sparse matrix} of shape (n_samples, n_features)
      :param y: The target values (class labels in classification, real numbers in
                regression).
      :type y: array-like of shape (n_samples,)
      :param sample_weight: Sample weights. If None, then samples are equally weighted.
                            Note that this is supported only if the base estimator supports
                            sample weighting.
      :type sample_weight: array-like of shape (n_samples,), default=None

      :returns: **self** -- Fitted estimator.
      :rtype: object



.. py:class:: BaseForestClassifier(base_estimator, n_estimators=100, estimator_params=tuple(), *, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)

   Bases: :py:obj:`ForestMixin`, :py:obj:`BaggingClassifier`


.. py:class:: BaseForestRegressor(*, base_estimator, estimator_params=tuple(), oob_score=False, n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, criterion='squared_error', bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`ForestMixin`, :py:obj:`BaggingRegressor`

   Base class for all estimators in scikit-learn.

   .. rubric:: Notes

   All estimators should specify all the parameters that can be set
   at the class level in their ``__init__`` as explicit keyword
   arguments (no ``*args`` or ``**kwargs``).


.. py:class:: BaseShapeletForestClassifier(base_estimator, n_estimators=100, estimator_params=tuple(), oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, n_shapelets='warn', min_shapelet_size=0.0, max_shapelet_size=1.0, metric='euclidean', metric_params=None, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)

   Bases: :py:obj:`BaseForestClassifier`

   Base class for shapelet forest classifiers.

   .. warning::

      This class should not be used directly. Use derived classes
      instead.


.. py:class:: BaseShapeletForestRegressor(*, base_estimator, estimator_params=tuple(), oob_score=False, n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, n_shapelets='warn', min_shapelet_size=0.0, max_shapelet_size=1.0, metric='euclidean', metric_params=None, criterion='squared_error', bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseForestRegressor`

   Base class for shapelet forest classifiers.

   .. warning::

      This class should not be used directly. Use derived classes
      instead.


.. py:class:: ExtraShapeletTreesClassifier(n_estimators=100, *, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, metric='euclidean', metric_params=None, criterion='entropy', oob_score=False, bootstrap=True, warm_start=False, class_weight=None, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestClassifier`

   An ensemble of extremely random shapelet trees for time series regression.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ExtraShapeletTreesClassifier
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ExtraShapeletTreesClassifier(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Construct a extra shapelet trees classifier.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param min_samples_leaf: The minimum number of samples in a leaf
   :type min_samples_leaf: int, optional
   :param criterion: The criterion used to evaluate the utility of a split
   :type criterion: {"entropy", "gini"}, optional
   :param min_impurity_decrease: A split will be introduced only if the impurity decrease is larger than or
                                 equal to this value
   :type min_impurity_decrease: float, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param class_weight: Weights associated with the labels

                        - if dict, weights on the form {label: weight}
                        - if "balanced" each class weight inversely proportional to the class
                          frequency
                        - if None, each class has equal weight
   :type class_weight: dict or "balanced", optional
   :param random_state:
                        - If `int`, `random_state` is the seed used by the random number generator
                        - If `RandomState` instance, `random_state` is the random number generator
                        - If `None`, the random number generator is the `RandomState` instance used
                          by `np.random`.
   :type random_state: int or RandomState


.. py:class:: ExtraShapeletTreesRegressor(n_estimators=100, *, max_depth=None, min_samples_split=2, min_shapelet_size=0, max_shapelet_size=1, metric='euclidean', metric_params=None, criterion='squared_error', oob_score=False, bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestRegressor`

   An ensemble of extremely random shapelet trees for time series regression.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ExtraShapeletTreesRegressor
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ExtraShapeletTreesRegressor(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Construct a extra shapelet trees regressor.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param random_state:
                        - If `int`, `random_state` is the seed used by the random number generator
                        - If `RandomState` instance, `random_state` is the random number generator
                        - If `None`, the random number generator is the `RandomState` instance used
                          by `np.random`.
   :type random_state: int or RandomState


.. py:class:: ForestMixin

   .. py:method:: apply(x)


   .. py:method:: decision_path(x)



.. py:class:: IntervalForestClassifier(n_estimators=100, *, n_intervals='sqrt', intervals='fixed', summarizer='auto', sample_size=0.5, min_size=0.0, max_size=1.0, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)

   Bases: :py:obj:`BaseForestClassifier`

   An ensemble of interval tree classifiers.


.. py:class:: IntervalForestRegressor(n_estimators=100, *, n_intervals='sqrt', intervals='fixed', summarizer='auto', sample_size=0.5, min_size=0.0, max_size=1.0, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='squared_error', bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseForestRegressor`

   An ensemble of interval tree regressors.


.. py:class:: IsolationShapeletForest(*, n_shapelets=1, n_estimators=100, bootstrap=False, n_jobs=None, min_shapelet_size=0, max_shapelet_size=1, min_samples_split=2, max_samples='auto', contamination='auto', warm_start=False, metric='euclidean', metric_params=None, random_state=None)

   Bases: :py:obj:`sklearn.base.OutlierMixin`, :py:obj:`ForestMixin`, :py:obj:`BaseBagging`

   A isolation shapelet forest.

   .. versionadded:: 0.3.5

   .. attribute:: offset_

      The offset for computing the final decision

      :type: float

   .. rubric:: Examples

   >>> from wildboar.ensemble import IsolationShapeletForest
   >>> from wildboar.datasets import load_two_lead_ecg
   >>> from wildboar.model_selection.outlier import train_test_split
   >>> from sklearn.metrics import balanced_accuracy_score
   >>> x, y = load_two_lead_ecg()
   >>> x_train, x_test, y_train, y_test = train_test_split(
   ...    x, y, 1, test_size=0.2, anomalies_train_size=0.05
   ... )
   >>> f = IsolationShapeletForest(
   ...     n_estimators=100, contamination=balanced_accuracy_score
   ... )
   >>> f.fit(x_train, y_train)
   >>> y_pred = f.predict(x_test)
   >>> balanced_accuracy_score(y_test, y_pred)

   Or using default offset threshold

   >>> from wildboar.ensemble import IsolationShapeletForest
   >>> from wildboar.datasets import load_two_lead_ecg
   >>> from wildboar.model_selection.outlier import train_test_split
   >>> from sklearn.metrics import balanced_accuracy_score
   >>> f = IsolationShapeletForest()
   >>> x, y = load_two_lead_ecg()
   >>> x_train, x_test, y_train, y_test = train_test_split(
   ...     x, y, 1, test_size=0.2, anomalies_train_size=0.05
   ... )
   >>> f.fit(x_train)
   >>> y_pred = f.predict(x_test)
   >>> balanced_accuracy_score(y_test, y_pred)

   Construct a shapelet isolation forest

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param max_samples: The number of samples to draw to train each base estimator
   :type max_samples: "auto", float or int, optional
   :param contamination: The strategy for computing the offset (see `offset_`)

                         - if 'auto', `offset_=-0.5`

                         - if float ``offset_`` is computed as the c:th percentile of scores.

                         If `bootstrap=True`, out-of-bag samples are used for computing the scores.
   :type contamination: 'auto' or float, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit and add
                      more estimators to the ensemble, otherwise, just fit a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param random_state:
                        - If `int`, `random_state` is the seed used by the random number generator
                        - If `RandomState` instance, `random_state` is the random number generator
                        - If `None`, the random number generator is the `RandomState` instance used
                          by `np.random`.
   :type random_state: int or RandomState

   .. py:method:: decision_function(x)


   .. py:method:: fit(x, y=None, sample_weight=None)

      Build a Bagging ensemble of estimators from the training set (X, y).

      :param X: The training input samples. Sparse matrices are accepted only if
                they are supported by the base estimator.
      :type X: {array-like, sparse matrix} of shape (n_samples, n_features)
      :param y: The target values (class labels in classification, real numbers in
                regression).
      :type y: array-like of shape (n_samples,)
      :param sample_weight: Sample weights. If None, then samples are equally weighted.
                            Note that this is supported only if the base estimator supports
                            sample weighting.
      :type sample_weight: array-like of shape (n_samples,), default=None

      :returns: **self** -- Fitted estimator.
      :rtype: object


   .. py:method:: predict(x)


   .. py:method:: score_samples(x)



.. py:class:: PivotForestClassifier(n_estimators=100, *, n_pivot='sqrt', metrics='all', oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)

   Bases: :py:obj:`BaseForestClassifier`

   An ensemble of interval tree classifiers.


.. py:class:: ProximityForestClassifier(n_estimators=100, *, n_pivot=1, pivot_sample='label', metric_sample='weighted', metric_factories='default', oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)

   Bases: :py:obj:`BaseForestClassifier`

   A forest of proximity trees

   .. rubric:: References

   Lucas, Benjamin, Ahmed Shifaz, Charlotte Pelletier, Lachlan O'Neill, Nayyar Zaidi,     Bart Goethals, François Petitjean, and Geoffrey I. Webb. (2019)
       Proximity forest: an effective and scalable distance-based classifier for time
       series. Data Mining and Knowledge Discovery


.. py:class:: RockestClassifier(n_estimators=100, *, n_kernels=10, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, sampling='auto', sampling_params=None, kernel_size=None, bias_prob=1.0, normalize_prob=1.0, padding_prob=0.5, criterion='entropy', bootstrap=True, warm_start=False, class_weight=None, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseForestClassifier`

   An ensemble of rocket tree classifiers.


.. py:class:: RockestRegressor(n_estimators=100, *, n_kernels=10, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, sampling='auto', sampling_params=None, kernel_size=None, bias_prob=1.0, normalize_prob=1.0, padding_prob=0.5, criterion='squared_error', bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseForestRegressor`

   An ensemble of rocket tree regressors.


.. py:class:: ShapeletForestClassifier(n_estimators=100, *, n_shapelets='warn', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, alpha=None, metric='euclidean', metric_params=None, criterion='entropy', oob_score=False, bootstrap=True, warm_start=False, class_weight=None, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestClassifier`

   An ensemble of random shapelet tree classifiers.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ShapeletForestClassifier
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ShapeletForestClassifier(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Shapelet forest classifier.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param n_shapelets: The number of shapelets to sample at each node
   :type n_shapelets: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param alpha: Dynamically decrease the number of sampled shapelets at each node according
                 to the current depth.

                 .. math:`w = 1 - e^{-|alpha| * depth})`

                 - if :math:`alpha < 0`, the number of sampled shapelets decrease from
                   ``n_shapelets`` towards 1 with increased depth.

                   .. math:`n_shapelets * (1 - w)`

                 - if :math:`alpha > 0`, the number of sampled shapelets increase from ``1``
                   towards ``n_shapelets`` with increased depth.

                   .. math:`n_shapelets * w`

                 - if ``None``, the number of sampled shapelets are the same independeth of
                   depth.
   :type alpha: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param min_samples_leaf: The minimum number of samples in a leaf
   :type min_samples_leaf: int, optional
   :param criterion: The criterion used to evaluate the utility of a split
   :type criterion: {"entropy", "gini"}, optional
   :param min_impurity_decrease: A split will be introduced only if the impurity decrease is larger than or
                                 equal to this value
   :type min_impurity_decrease: float, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param oob_score: Compute out-of-bag estimates of the ensembles performance.
   :type oob_score: bool, optional
   :param class_weight: Weights associated with the labels

                        - if dict, weights on the form {label: weight}
                        - if "balanced" each class weight inversely proportional to the class
                          frequency
                        - if None, each class has equal weight
   :type class_weight: dict or "balanced", optional
   :param random_state:
                        - If `int`, `random_state` is the seed used by the random number generator
                        - If `RandomState` instance, `random_state` is the random number generator
                        - If `None`, the random number generator is the `RandomState` instance used
                          by `np.random`.
   :type random_state: int or RandomState


.. py:class:: ShapeletForestEmbedding(n_estimators=100, *, n_shapelets=1, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, metric='euclidean', metric_params=None, criterion='squared_error', bootstrap=True, warm_start=False, n_jobs=None, sparse_output=True, random_state=None)

   Bases: :py:obj:`BaseShapeletForestRegressor`

   An ensemble of random shapelet trees

   An unsupervised transformation of a time series dataset
   to a high-dimensional sparse representation. A time series i
   indexed by the leaf that it falls into. This leads to a binary
   coding of a time series with as many ones as trees in the forest.

   The dimensionality of the resulting representation is
   ``<= n_estimators * 2^max_depth``


   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param sparse_output: Return a sparse CSR-matrix.
   :type sparse_output: bool, optional
   :param random_state:
                        - If `int`, `random_state` is the seed used by the random number generator
                        - If `RandomState` instance, `random_state` is the random number generator
                        - If `None`, the random number generator is the `RandomState` instance used
                          by `np.random`.
   :type random_state: int or RandomState

   .. py:method:: fit(x, y=None, sample_weight=None)

      Build a Bagging ensemble of estimators from the training set (X, y).

      :param X: The training input samples. Sparse matrices are accepted only if
                they are supported by the base estimator.
      :type X: {array-like, sparse matrix} of shape (n_samples, n_features)
      :param y: The target values (class labels in classification, real numbers in
                regression).
      :type y: array-like of shape (n_samples,)
      :param sample_weight: Sample weights. If None, then samples are equally weighted.
                            Note that this is supported only if the base estimator supports
                            sample weighting.
      :type sample_weight: array-like of shape (n_samples,), default=None

      :returns: **self** -- Fitted estimator.
      :rtype: object


   .. py:method:: fit_transform(x, y=None, sample_weight=None)


   .. py:method:: transform(x)



.. py:class:: ShapeletForestRegressor(n_estimators=100, *, n_shapelets='warn', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, alpha=None, metric='euclidean', metric_params=None, criterion='squared_error', oob_score=False, bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestRegressor`

   An ensemble of random shapelet regression trees.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ShapeletForestRegressor
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ShapeletForestRegressor(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Shapelet forest regressor.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param n_shapelets: The number of shapelets to sample at each node
   :type n_shapelets: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param alpha: Dynamically decrease the number of sampled shapelets at each node according
                 to the current depth.

                 .. math:`w = 1 - e^{-|alpha| * depth})`

                 - if :math:`alpha < 0`, the number of sampled shapelets decrease from
                   ``n_shapelets`` towards 1 with increased depth.

                   .. math:`n_shapelets * (1 - w)`

                 - if :math:`alpha > 0`, the number of sampled shapelets increase from ``1``
                   towards ``n_shapelets`` with increased depth.

                   .. math:`n_shapelets * w`

                 - if ``None``, the number of sampled shapelets are the same independeth of
                   depth.
   :type alpha: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param oob_score: Compute out-of-bag estimates of the ensembles performance.
   :type oob_score: bool, optional
   :param random_state:
                        - If `int`, `random_state` is the seed used by the random number generator
                        - If `RandomState` instance, `random_state` is the random number generator
                        - If `None`, the random number generator is the `RandomState` instance used
                          by `np.random`.
   :type random_state: int or RandomState


