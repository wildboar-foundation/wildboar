:py:mod:`wildboar.ensemble`
===========================

.. py:module:: wildboar.ensemble


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   wildboar.ensemble.ExtraShapeletTreesClassifier
   wildboar.ensemble.ExtraShapeletTreesRegressor
   wildboar.ensemble.IntervalForestClassifier
   wildboar.ensemble.IntervalForestRegressor
   wildboar.ensemble.IsolationShapeletForest
   wildboar.ensemble.PivotForestClassifier
   wildboar.ensemble.ProximityForestClassifier
   wildboar.ensemble.RockestClassifier
   wildboar.ensemble.RockestRegressor
   wildboar.ensemble.ShapeletForestClassifier
   wildboar.ensemble.ShapeletForestEmbedding
   wildboar.ensemble.ShapeletForestRegressor




.. py:class:: ExtraShapeletTreesClassifier(n_estimators=100, *, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, metric='euclidean', metric_params=None, criterion='entropy', oob_score=False, bootstrap=True, warm_start=False, class_weight=None, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestClassifier`

   An ensemble of extremely random shapelet trees for time series regression.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ExtraShapeletTreesClassifier
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ExtraShapeletTreesClassifier(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Construct a extra shapelet trees classifier.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param min_samples_leaf: The minimum number of samples in a leaf
   :type min_samples_leaf: int, optional
   :param criterion: The criterion used to evaluate the utility of a split
   :type criterion: {"entropy", "gini"}, optional
   :param min_impurity_decrease: A split will be introduced only if the impurity decrease is larger than or
                                 equal to this value
   :type min_impurity_decrease: float, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param class_weight: Weights associated with the labels

                        - if dict, weights on the form {label: weight}
                        - if "balanced" each class weight inversely proportional to the class
                          frequency
                        - if None, each class has equal weight
   :type class_weight: dict or "balanced", optional
   :param random_state: Controls the random resampling of the original dataset and the construction
                        of the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional


.. py:class:: ExtraShapeletTreesRegressor(n_estimators=100, *, max_depth=None, min_samples_split=2, min_shapelet_size=0, max_shapelet_size=1, metric='euclidean', metric_params=None, criterion='mse', oob_score=False, bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestRegressor`

   An ensemble of extremely random shapelet trees for time series regression.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ExtraShapeletTreesRegressor
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ExtraShapeletTreesRegressor(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Construct a extra shapelet trees regressor.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param random_state: Controls the random resampling of the original dataset and the construction
                        of the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional


.. py:class:: IntervalForestClassifier(n_estimators=100, *, n_interval='sqrt', intervals='fixed', summarizer='auto', sample_size=0.5, min_size=0.0, max_size=1.0, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)

   Bases: :py:obj:`BaseForestClassifier`

   A Bagging classifier.

   A Bagging classifier is an ensemble meta-estimator that fits base
   classifiers each on random subsets of the original dataset and then
   aggregate their individual predictions (either by voting or by averaging)
   to form a final prediction. Such a meta-estimator can typically be used as
   a way to reduce the variance of a black-box estimator (e.g., a decision
   tree), by introducing randomization into its construction procedure and
   then making an ensemble out of it.

   This algorithm encompasses several works from the literature. When random
   subsets of the dataset are drawn as random subsets of the samples, then
   this algorithm is known as Pasting [1]_. If samples are drawn with
   replacement, then the method is known as Bagging [2]_. When random subsets
   of the dataset are drawn as random subsets of the features, then the method
   is known as Random Subspaces [3]_. Finally, when base estimators are built
   on subsets of both samples and features, then the method is known as
   Random Patches [4]_.

   Read more in the :ref:`User Guide <bagging>`.

   .. versionadded:: 0.15

   :param base_estimator: The base estimator to fit on random subsets of the dataset.
                          If None, then the base estimator is a
                          :class:`~sklearn.tree.DecisionTreeClassifier`.
   :type base_estimator: object, default=None
   :param n_estimators: The number of base estimators in the ensemble.
   :type n_estimators: int, default=10
   :param max_samples: The number of samples to draw from X to train each base estimator (with
                       replacement by default, see `bootstrap` for more details).

                       - If int, then draw `max_samples` samples.
                       - If float, then draw `max_samples * X.shape[0]` samples.
   :type max_samples: int or float, default=1.0
   :param max_features: The number of features to draw from X to train each base estimator (
                        without replacement by default, see `bootstrap_features` for more
                        details).

                        - If int, then draw `max_features` features.
                        - If float, then draw `max_features * X.shape[1]` features.
   :type max_features: int or float, default=1.0
   :param bootstrap: Whether samples are drawn with replacement. If False, sampling
                     without replacement is performed.
   :type bootstrap: bool, default=True
   :param bootstrap_features: Whether features are drawn with replacement.
   :type bootstrap_features: bool, default=False
   :param oob_score: Whether to use out-of-bag samples to estimate
                     the generalization error. Only available if bootstrap=True.
   :type oob_score: bool, default=False
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble. See :term:`the Glossary <warm_start>`.

                      .. versionadded:: 0.17
                         *warm_start* constructor parameter.
   :type warm_start: bool, default=False
   :param n_jobs: The number of jobs to run in parallel for both :meth:`fit` and
                  :meth:`predict`. ``None`` means 1 unless in a
                  :obj:`joblib.parallel_backend` context. ``-1`` means using all
                  processors. See :term:`Glossary <n_jobs>` for more details.
   :type n_jobs: int, default=None
   :param random_state: Controls the random resampling of the original dataset
                        (sample wise and feature wise).
                        If the base estimator accepts a `random_state` attribute, a different
                        seed is generated for each instance in the ensemble.
                        Pass an int for reproducible output across multiple function calls.
                        See :term:`Glossary <random_state>`.
   :type random_state: int, RandomState instance or None, default=None
   :param verbose: Controls the verbosity when fitting and predicting.
   :type verbose: int, default=0

   .. attribute:: base_estimator_

      The base estimator from which the ensemble is grown.

      :type: estimator

   .. attribute:: n_features_

      The number of features when :meth:`fit` is performed.

      .. deprecated:: 1.0
          Attribute `n_features_` was deprecated in version 1.0 and will be
          removed in 1.2. Use `n_features_in_` instead.

      :type: int

   .. attribute:: n_features_in_

      Number of features seen during :term:`fit`.

      .. versionadded:: 0.24

      :type: int

   .. attribute:: feature_names_in_

      Names of features seen during :term:`fit`. Defined only when `X`
      has feature names that are all strings.

      .. versionadded:: 1.0

      :type: ndarray of shape (`n_features_in_`,)

   .. attribute:: estimators_

      The collection of fitted base estimators.

      :type: list of estimators

   .. attribute:: estimators_samples_

      The subset of drawn samples (i.e., the in-bag samples) for each base
      estimator. Each subset is defined by an array of the indices selected.

      :type: list of arrays

   .. attribute:: estimators_features_

      The subset of drawn features for each base estimator.

      :type: list of arrays

   .. attribute:: classes_

      The classes labels.

      :type: ndarray of shape (n_classes,)

   .. attribute:: n_classes_

      The number of classes.

      :type: int or list

   .. attribute:: oob_score_

      Score of the training dataset obtained using an out-of-bag estimate.
      This attribute exists only when ``oob_score`` is True.

      :type: float

   .. attribute:: oob_decision_function_

      Decision function computed with out-of-bag estimate on the training
      set. If n_estimators is small it might be possible that a data point
      was never left out during the bootstrap. In this case,
      `oob_decision_function_` might contain NaN. This attribute exists
      only when ``oob_score`` is True.

      :type: ndarray of shape (n_samples, n_classes)

   .. seealso::

      :obj:`BaggingRegressor`
          A Bagging regressor.

   .. rubric:: References

   .. [1] L. Breiman, "Pasting small votes for classification in large
          databases and on-line", Machine Learning, 36(1), 85-103, 1999.

   .. [2] L. Breiman, "Bagging predictors", Machine Learning, 24(2), 123-140,
          1996.

   .. [3] T. Ho, "The random subspace method for constructing decision
          forests", Pattern Analysis and Machine Intelligence, 20(8), 832-844,
          1998.

   .. [4] G. Louppe and P. Geurts, "Ensembles on Random Patches", Machine
          Learning and Knowledge Discovery in Databases, 346-361, 2012.

   .. rubric:: Examples

   >>> from sklearn.svm import SVC
   >>> from sklearn.ensemble import BaggingClassifier
   >>> from sklearn.datasets import make_classification
   >>> X, y = make_classification(n_samples=100, n_features=4,
   ...                            n_informative=2, n_redundant=0,
   ...                            random_state=0, shuffle=False)
   >>> clf = BaggingClassifier(base_estimator=SVC(),
   ...                         n_estimators=10, random_state=0).fit(X, y)
   >>> clf.predict([[0, 0, 0, 0]])
   array([1])


.. py:class:: IntervalForestRegressor(n_estimators=100, *, n_interval='sqrt', intervals='fixed', summarizer='auto', sample_size=0.5, min_size=0.0, max_size=1.0, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='mse', bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseForestRegressor`

   A Bagging regressor.

   A Bagging regressor is an ensemble meta-estimator that fits base
   regressors each on random subsets of the original dataset and then
   aggregate their individual predictions (either by voting or by averaging)
   to form a final prediction. Such a meta-estimator can typically be used as
   a way to reduce the variance of a black-box estimator (e.g., a decision
   tree), by introducing randomization into its construction procedure and
   then making an ensemble out of it.

   This algorithm encompasses several works from the literature. When random
   subsets of the dataset are drawn as random subsets of the samples, then
   this algorithm is known as Pasting [1]_. If samples are drawn with
   replacement, then the method is known as Bagging [2]_. When random subsets
   of the dataset are drawn as random subsets of the features, then the method
   is known as Random Subspaces [3]_. Finally, when base estimators are built
   on subsets of both samples and features, then the method is known as
   Random Patches [4]_.

   Read more in the :ref:`User Guide <bagging>`.

   .. versionadded:: 0.15

   :param base_estimator: The base estimator to fit on random subsets of the dataset.
                          If None, then the base estimator is a
                          :class:`~sklearn.tree.DecisionTreeRegressor`.
   :type base_estimator: object, default=None
   :param n_estimators: The number of base estimators in the ensemble.
   :type n_estimators: int, default=10
   :param max_samples: The number of samples to draw from X to train each base estimator (with
                       replacement by default, see `bootstrap` for more details).

                       - If int, then draw `max_samples` samples.
                       - If float, then draw `max_samples * X.shape[0]` samples.
   :type max_samples: int or float, default=1.0
   :param max_features: The number of features to draw from X to train each base estimator (
                        without replacement by default, see `bootstrap_features` for more
                        details).

                        - If int, then draw `max_features` features.
                        - If float, then draw `max_features * X.shape[1]` features.
   :type max_features: int or float, default=1.0
   :param bootstrap: Whether samples are drawn with replacement. If False, sampling
                     without replacement is performed.
   :type bootstrap: bool, default=True
   :param bootstrap_features: Whether features are drawn with replacement.
   :type bootstrap_features: bool, default=False
   :param oob_score: Whether to use out-of-bag samples to estimate
                     the generalization error. Only available if bootstrap=True.
   :type oob_score: bool, default=False
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble. See :term:`the Glossary <warm_start>`.
   :type warm_start: bool, default=False
   :param n_jobs: The number of jobs to run in parallel for both :meth:`fit` and
                  :meth:`predict`. ``None`` means 1 unless in a
                  :obj:`joblib.parallel_backend` context. ``-1`` means using all
                  processors. See :term:`Glossary <n_jobs>` for more details.
   :type n_jobs: int, default=None
   :param random_state: Controls the random resampling of the original dataset
                        (sample wise and feature wise).
                        If the base estimator accepts a `random_state` attribute, a different
                        seed is generated for each instance in the ensemble.
                        Pass an int for reproducible output across multiple function calls.
                        See :term:`Glossary <random_state>`.
   :type random_state: int, RandomState instance or None, default=None
   :param verbose: Controls the verbosity when fitting and predicting.
   :type verbose: int, default=0

   .. attribute:: base_estimator_

      The base estimator from which the ensemble is grown.

      :type: estimator

   .. attribute:: n_features_

      The number of features when :meth:`fit` is performed.

      .. deprecated:: 1.0
          Attribute `n_features_` was deprecated in version 1.0 and will be
          removed in 1.2. Use `n_features_in_` instead.

      :type: int

   .. attribute:: n_features_in_

      Number of features seen during :term:`fit`.

      .. versionadded:: 0.24

      :type: int

   .. attribute:: feature_names_in_

      Names of features seen during :term:`fit`. Defined only when `X`
      has feature names that are all strings.

      .. versionadded:: 1.0

      :type: ndarray of shape (`n_features_in_`,)

   .. attribute:: estimators_

      The collection of fitted sub-estimators.

      :type: list of estimators

   .. attribute:: estimators_samples_

      The subset of drawn samples (i.e., the in-bag samples) for each base
      estimator. Each subset is defined by an array of the indices selected.

      :type: list of arrays

   .. attribute:: estimators_features_

      The subset of drawn features for each base estimator.

      :type: list of arrays

   .. attribute:: oob_score_

      Score of the training dataset obtained using an out-of-bag estimate.
      This attribute exists only when ``oob_score`` is True.

      :type: float

   .. attribute:: oob_prediction_

      Prediction computed with out-of-bag estimate on the training
      set. If n_estimators is small it might be possible that a data point
      was never left out during the bootstrap. In this case,
      `oob_prediction_` might contain NaN. This attribute exists only
      when ``oob_score`` is True.

      :type: ndarray of shape (n_samples,)

   .. seealso::

      :obj:`BaggingClassifier`
          A Bagging classifier.

   .. rubric:: References

   .. [1] L. Breiman, "Pasting small votes for classification in large
          databases and on-line", Machine Learning, 36(1), 85-103, 1999.

   .. [2] L. Breiman, "Bagging predictors", Machine Learning, 24(2), 123-140,
          1996.

   .. [3] T. Ho, "The random subspace method for constructing decision
          forests", Pattern Analysis and Machine Intelligence, 20(8), 832-844,
          1998.

   .. [4] G. Louppe and P. Geurts, "Ensembles on Random Patches", Machine
          Learning and Knowledge Discovery in Databases, 346-361, 2012.

   .. rubric:: Examples

   >>> from sklearn.svm import SVR
   >>> from sklearn.ensemble import BaggingRegressor
   >>> from sklearn.datasets import make_regression
   >>> X, y = make_regression(n_samples=100, n_features=4,
   ...                        n_informative=2, n_targets=1,
   ...                        random_state=0, shuffle=False)
   >>> regr = BaggingRegressor(base_estimator=SVR(),
   ...                         n_estimators=10, random_state=0).fit(X, y)
   >>> regr.predict([[0, 0, 0, 0]])
   array([-2.8720...])


.. py:class:: IsolationShapeletForest(*, n_estimators=100, bootstrap=False, n_jobs=None, min_shapelet_size=0, max_shapelet_size=1, min_samples_split=2, max_samples='auto', contamination='auto', contamination_set='training', warm_start=False, metric='euclidean', metric_params=None, random_state=None)

   Bases: :py:obj:`ForestMixin`, :py:obj:`sklearn.base.OutlierMixin`, :py:obj:`sklearn.ensemble._bagging.BaseBagging`

   A isolation shapelet forest.

   .. versionadded:: 0.3.5

   .. attribute:: offset_

      The offset for computing the final decision

      :type: float

   .. rubric:: Examples

   >>> from wildboar.ensemble import IsolationShapeletForest
   >>> from wildboar.datasets import load_two_lead_ecg
   >>> from wildboar.model_selection.outlier import train_test_split
   >>> from sklearn.metrics import balanced_accuracy_score
   >>> x, y = load_two_lead_ecg()
   >>> x_train, x_test, y_train, y_test = train_test_split(
   ...    x, y, 1, test_size=0.2, anomalies_train_size=0.05
   ... )
   >>> f = IsolationShapeletForest(
   ...     n_estimators=100, contamination=balanced_accuracy_score
   ... )
   >>> f.fit(x_train, y_train)
   >>> y_pred = f.predict(x_test)
   >>> balanced_accuracy_score(y_test, y_pred)

   Or using default offset threshold

   >>> from wildboar.ensemble import IsolationShapeletForest
   >>> from wildboar.datasets import load_two_lead_ecg
   >>> from wildboar.model_selection.outlier import train_test_split
   >>> from sklearn.metrics import balanced_accuracy_score
   >>> f = IsolationShapeletForest()
   >>> x, y = load_two_lead_ecg()
   >>> x_train, x_test, y_train, y_test = train_test_split(
   ...     x, y, 1, test_size=0.2, anomalies_train_size=0.05
   ... )
   >>> f.fit(x_train)
   >>> y_pred = f.predict(x_test)
   >>> balanced_accuracy_score(y_test, y_pred)

   Construct a shapelet isolation forest

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param max_samples: The number of samples to draw to train each base estimator
   :type max_samples: float or int
   :param contamination: The strategy for computing the offset (see `offset_`)

                         - if 'auto' ``offset_=-0.5``
                         - if 'auc' ``offset_`` is computed as the offset that maximizes the
                           area under ROC in the training or out-of-bag set
                           (see ``contamination_set``).
                         - if 'prc' ``offset_`` is computed as the offset that maximizes the
                           area under PRC in the training or out-of-bag set
                           (see ``contamination_set``)
                         - if callable ``offset_`` is computed as the offset that maximizes the score
                           computed by the callable in training or out-of-bag set
                           (see ``contamination_set``)
                         - if float ``offset_`` is computed as the c:th percentile of scores in the
                           training or out-of-bag set (see ``contamination_set``)

                         Setting contamination to either 'auc' or 'prc' require that `y` is passed
                         to `fit`.
   :type contamination: str, float or callable
   :param contamination_set: Compute the ``offset_`` from either the out-of-bag samples or the training
                             samples.'oob' require `bootstrap=True`.
   :type contamination_set: {'training', 'oob'}, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param random_state: Controls the random resampling of the original dataset and the construction
                        of the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional

   .. py:method:: decision_function(self, x)


   .. py:method:: fit(self, x, y=None, sample_weight=None, check_input=True)

      Build a Bagging ensemble of estimators from the training set (X, y).

      :param X: The training input samples. Sparse matrices are accepted only if
                they are supported by the base estimator.
      :type X: {array-like, sparse matrix} of shape (n_samples, n_features)
      :param y: The target values (class labels in classification, real numbers in
                regression).
      :type y: array-like of shape (n_samples,)
      :param sample_weight: Sample weights. If None, then samples are equally weighted.
                            Note that this is supported only if the base estimator supports
                            sample weighting.
      :type sample_weight: array-like of shape (n_samples,), default=None

      :returns: **self** -- Fitted estimator.
      :rtype: object


   .. py:method:: predict(self, x)


   .. py:method:: score_samples(self, x)



.. py:class:: PivotForestClassifier(n_estimators=100, *, n_pivot='sqrt', metrics='all', oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)

   Bases: :py:obj:`BaseForestClassifier`

   A Bagging classifier.

   A Bagging classifier is an ensemble meta-estimator that fits base
   classifiers each on random subsets of the original dataset and then
   aggregate their individual predictions (either by voting or by averaging)
   to form a final prediction. Such a meta-estimator can typically be used as
   a way to reduce the variance of a black-box estimator (e.g., a decision
   tree), by introducing randomization into its construction procedure and
   then making an ensemble out of it.

   This algorithm encompasses several works from the literature. When random
   subsets of the dataset are drawn as random subsets of the samples, then
   this algorithm is known as Pasting [1]_. If samples are drawn with
   replacement, then the method is known as Bagging [2]_. When random subsets
   of the dataset are drawn as random subsets of the features, then the method
   is known as Random Subspaces [3]_. Finally, when base estimators are built
   on subsets of both samples and features, then the method is known as
   Random Patches [4]_.

   Read more in the :ref:`User Guide <bagging>`.

   .. versionadded:: 0.15

   :param base_estimator: The base estimator to fit on random subsets of the dataset.
                          If None, then the base estimator is a
                          :class:`~sklearn.tree.DecisionTreeClassifier`.
   :type base_estimator: object, default=None
   :param n_estimators: The number of base estimators in the ensemble.
   :type n_estimators: int, default=10
   :param max_samples: The number of samples to draw from X to train each base estimator (with
                       replacement by default, see `bootstrap` for more details).

                       - If int, then draw `max_samples` samples.
                       - If float, then draw `max_samples * X.shape[0]` samples.
   :type max_samples: int or float, default=1.0
   :param max_features: The number of features to draw from X to train each base estimator (
                        without replacement by default, see `bootstrap_features` for more
                        details).

                        - If int, then draw `max_features` features.
                        - If float, then draw `max_features * X.shape[1]` features.
   :type max_features: int or float, default=1.0
   :param bootstrap: Whether samples are drawn with replacement. If False, sampling
                     without replacement is performed.
   :type bootstrap: bool, default=True
   :param bootstrap_features: Whether features are drawn with replacement.
   :type bootstrap_features: bool, default=False
   :param oob_score: Whether to use out-of-bag samples to estimate
                     the generalization error. Only available if bootstrap=True.
   :type oob_score: bool, default=False
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble. See :term:`the Glossary <warm_start>`.

                      .. versionadded:: 0.17
                         *warm_start* constructor parameter.
   :type warm_start: bool, default=False
   :param n_jobs: The number of jobs to run in parallel for both :meth:`fit` and
                  :meth:`predict`. ``None`` means 1 unless in a
                  :obj:`joblib.parallel_backend` context. ``-1`` means using all
                  processors. See :term:`Glossary <n_jobs>` for more details.
   :type n_jobs: int, default=None
   :param random_state: Controls the random resampling of the original dataset
                        (sample wise and feature wise).
                        If the base estimator accepts a `random_state` attribute, a different
                        seed is generated for each instance in the ensemble.
                        Pass an int for reproducible output across multiple function calls.
                        See :term:`Glossary <random_state>`.
   :type random_state: int, RandomState instance or None, default=None
   :param verbose: Controls the verbosity when fitting and predicting.
   :type verbose: int, default=0

   .. attribute:: base_estimator_

      The base estimator from which the ensemble is grown.

      :type: estimator

   .. attribute:: n_features_

      The number of features when :meth:`fit` is performed.

      .. deprecated:: 1.0
          Attribute `n_features_` was deprecated in version 1.0 and will be
          removed in 1.2. Use `n_features_in_` instead.

      :type: int

   .. attribute:: n_features_in_

      Number of features seen during :term:`fit`.

      .. versionadded:: 0.24

      :type: int

   .. attribute:: feature_names_in_

      Names of features seen during :term:`fit`. Defined only when `X`
      has feature names that are all strings.

      .. versionadded:: 1.0

      :type: ndarray of shape (`n_features_in_`,)

   .. attribute:: estimators_

      The collection of fitted base estimators.

      :type: list of estimators

   .. attribute:: estimators_samples_

      The subset of drawn samples (i.e., the in-bag samples) for each base
      estimator. Each subset is defined by an array of the indices selected.

      :type: list of arrays

   .. attribute:: estimators_features_

      The subset of drawn features for each base estimator.

      :type: list of arrays

   .. attribute:: classes_

      The classes labels.

      :type: ndarray of shape (n_classes,)

   .. attribute:: n_classes_

      The number of classes.

      :type: int or list

   .. attribute:: oob_score_

      Score of the training dataset obtained using an out-of-bag estimate.
      This attribute exists only when ``oob_score`` is True.

      :type: float

   .. attribute:: oob_decision_function_

      Decision function computed with out-of-bag estimate on the training
      set. If n_estimators is small it might be possible that a data point
      was never left out during the bootstrap. In this case,
      `oob_decision_function_` might contain NaN. This attribute exists
      only when ``oob_score`` is True.

      :type: ndarray of shape (n_samples, n_classes)

   .. seealso::

      :obj:`BaggingRegressor`
          A Bagging regressor.

   .. rubric:: References

   .. [1] L. Breiman, "Pasting small votes for classification in large
          databases and on-line", Machine Learning, 36(1), 85-103, 1999.

   .. [2] L. Breiman, "Bagging predictors", Machine Learning, 24(2), 123-140,
          1996.

   .. [3] T. Ho, "The random subspace method for constructing decision
          forests", Pattern Analysis and Machine Intelligence, 20(8), 832-844,
          1998.

   .. [4] G. Louppe and P. Geurts, "Ensembles on Random Patches", Machine
          Learning and Knowledge Discovery in Databases, 346-361, 2012.

   .. rubric:: Examples

   >>> from sklearn.svm import SVC
   >>> from sklearn.ensemble import BaggingClassifier
   >>> from sklearn.datasets import make_classification
   >>> X, y = make_classification(n_samples=100, n_features=4,
   ...                            n_informative=2, n_redundant=0,
   ...                            random_state=0, shuffle=False)
   >>> clf = BaggingClassifier(base_estimator=SVC(),
   ...                         n_estimators=10, random_state=0).fit(X, y)
   >>> clf.predict([[0, 0, 0, 0]])
   array([1])


.. py:class:: ProximityForestClassifier(n_estimators=100, *, n_pivot=1, pivot_sample='label', metric_sample='weighted', metrics=None, metrics_params=None, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)

   Bases: :py:obj:`BaseForestClassifier`

   A Bagging classifier.

   A Bagging classifier is an ensemble meta-estimator that fits base
   classifiers each on random subsets of the original dataset and then
   aggregate their individual predictions (either by voting or by averaging)
   to form a final prediction. Such a meta-estimator can typically be used as
   a way to reduce the variance of a black-box estimator (e.g., a decision
   tree), by introducing randomization into its construction procedure and
   then making an ensemble out of it.

   This algorithm encompasses several works from the literature. When random
   subsets of the dataset are drawn as random subsets of the samples, then
   this algorithm is known as Pasting [1]_. If samples are drawn with
   replacement, then the method is known as Bagging [2]_. When random subsets
   of the dataset are drawn as random subsets of the features, then the method
   is known as Random Subspaces [3]_. Finally, when base estimators are built
   on subsets of both samples and features, then the method is known as
   Random Patches [4]_.

   Read more in the :ref:`User Guide <bagging>`.

   .. versionadded:: 0.15

   :param base_estimator: The base estimator to fit on random subsets of the dataset.
                          If None, then the base estimator is a
                          :class:`~sklearn.tree.DecisionTreeClassifier`.
   :type base_estimator: object, default=None
   :param n_estimators: The number of base estimators in the ensemble.
   :type n_estimators: int, default=10
   :param max_samples: The number of samples to draw from X to train each base estimator (with
                       replacement by default, see `bootstrap` for more details).

                       - If int, then draw `max_samples` samples.
                       - If float, then draw `max_samples * X.shape[0]` samples.
   :type max_samples: int or float, default=1.0
   :param max_features: The number of features to draw from X to train each base estimator (
                        without replacement by default, see `bootstrap_features` for more
                        details).

                        - If int, then draw `max_features` features.
                        - If float, then draw `max_features * X.shape[1]` features.
   :type max_features: int or float, default=1.0
   :param bootstrap: Whether samples are drawn with replacement. If False, sampling
                     without replacement is performed.
   :type bootstrap: bool, default=True
   :param bootstrap_features: Whether features are drawn with replacement.
   :type bootstrap_features: bool, default=False
   :param oob_score: Whether to use out-of-bag samples to estimate
                     the generalization error. Only available if bootstrap=True.
   :type oob_score: bool, default=False
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble. See :term:`the Glossary <warm_start>`.

                      .. versionadded:: 0.17
                         *warm_start* constructor parameter.
   :type warm_start: bool, default=False
   :param n_jobs: The number of jobs to run in parallel for both :meth:`fit` and
                  :meth:`predict`. ``None`` means 1 unless in a
                  :obj:`joblib.parallel_backend` context. ``-1`` means using all
                  processors. See :term:`Glossary <n_jobs>` for more details.
   :type n_jobs: int, default=None
   :param random_state: Controls the random resampling of the original dataset
                        (sample wise and feature wise).
                        If the base estimator accepts a `random_state` attribute, a different
                        seed is generated for each instance in the ensemble.
                        Pass an int for reproducible output across multiple function calls.
                        See :term:`Glossary <random_state>`.
   :type random_state: int, RandomState instance or None, default=None
   :param verbose: Controls the verbosity when fitting and predicting.
   :type verbose: int, default=0

   .. attribute:: base_estimator_

      The base estimator from which the ensemble is grown.

      :type: estimator

   .. attribute:: n_features_

      The number of features when :meth:`fit` is performed.

      .. deprecated:: 1.0
          Attribute `n_features_` was deprecated in version 1.0 and will be
          removed in 1.2. Use `n_features_in_` instead.

      :type: int

   .. attribute:: n_features_in_

      Number of features seen during :term:`fit`.

      .. versionadded:: 0.24

      :type: int

   .. attribute:: feature_names_in_

      Names of features seen during :term:`fit`. Defined only when `X`
      has feature names that are all strings.

      .. versionadded:: 1.0

      :type: ndarray of shape (`n_features_in_`,)

   .. attribute:: estimators_

      The collection of fitted base estimators.

      :type: list of estimators

   .. attribute:: estimators_samples_

      The subset of drawn samples (i.e., the in-bag samples) for each base
      estimator. Each subset is defined by an array of the indices selected.

      :type: list of arrays

   .. attribute:: estimators_features_

      The subset of drawn features for each base estimator.

      :type: list of arrays

   .. attribute:: classes_

      The classes labels.

      :type: ndarray of shape (n_classes,)

   .. attribute:: n_classes_

      The number of classes.

      :type: int or list

   .. attribute:: oob_score_

      Score of the training dataset obtained using an out-of-bag estimate.
      This attribute exists only when ``oob_score`` is True.

      :type: float

   .. attribute:: oob_decision_function_

      Decision function computed with out-of-bag estimate on the training
      set. If n_estimators is small it might be possible that a data point
      was never left out during the bootstrap. In this case,
      `oob_decision_function_` might contain NaN. This attribute exists
      only when ``oob_score`` is True.

      :type: ndarray of shape (n_samples, n_classes)

   .. seealso::

      :obj:`BaggingRegressor`
          A Bagging regressor.

   .. rubric:: References

   .. [1] L. Breiman, "Pasting small votes for classification in large
          databases and on-line", Machine Learning, 36(1), 85-103, 1999.

   .. [2] L. Breiman, "Bagging predictors", Machine Learning, 24(2), 123-140,
          1996.

   .. [3] T. Ho, "The random subspace method for constructing decision
          forests", Pattern Analysis and Machine Intelligence, 20(8), 832-844,
          1998.

   .. [4] G. Louppe and P. Geurts, "Ensembles on Random Patches", Machine
          Learning and Knowledge Discovery in Databases, 346-361, 2012.

   .. rubric:: Examples

   >>> from sklearn.svm import SVC
   >>> from sklearn.ensemble import BaggingClassifier
   >>> from sklearn.datasets import make_classification
   >>> X, y = make_classification(n_samples=100, n_features=4,
   ...                            n_informative=2, n_redundant=0,
   ...                            random_state=0, shuffle=False)
   >>> clf = BaggingClassifier(base_estimator=SVC(),
   ...                         n_estimators=10, random_state=0).fit(X, y)
   >>> clf.predict([[0, 0, 0, 0]])
   array([1])


.. py:class:: RockestClassifier(n_estimators=100, *, n_kernels=10, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, sampling='auto', sampling_params=None, kernel_size=None, bias_prob=1.0, normalize_prob=1.0, padding_prob=0.5, criterion='entropy', bootstrap=True, warm_start=False, class_weight=None, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseForestClassifier`

   A Bagging classifier.

   A Bagging classifier is an ensemble meta-estimator that fits base
   classifiers each on random subsets of the original dataset and then
   aggregate their individual predictions (either by voting or by averaging)
   to form a final prediction. Such a meta-estimator can typically be used as
   a way to reduce the variance of a black-box estimator (e.g., a decision
   tree), by introducing randomization into its construction procedure and
   then making an ensemble out of it.

   This algorithm encompasses several works from the literature. When random
   subsets of the dataset are drawn as random subsets of the samples, then
   this algorithm is known as Pasting [1]_. If samples are drawn with
   replacement, then the method is known as Bagging [2]_. When random subsets
   of the dataset are drawn as random subsets of the features, then the method
   is known as Random Subspaces [3]_. Finally, when base estimators are built
   on subsets of both samples and features, then the method is known as
   Random Patches [4]_.

   Read more in the :ref:`User Guide <bagging>`.

   .. versionadded:: 0.15

   :param base_estimator: The base estimator to fit on random subsets of the dataset.
                          If None, then the base estimator is a
                          :class:`~sklearn.tree.DecisionTreeClassifier`.
   :type base_estimator: object, default=None
   :param n_estimators: The number of base estimators in the ensemble.
   :type n_estimators: int, default=10
   :param max_samples: The number of samples to draw from X to train each base estimator (with
                       replacement by default, see `bootstrap` for more details).

                       - If int, then draw `max_samples` samples.
                       - If float, then draw `max_samples * X.shape[0]` samples.
   :type max_samples: int or float, default=1.0
   :param max_features: The number of features to draw from X to train each base estimator (
                        without replacement by default, see `bootstrap_features` for more
                        details).

                        - If int, then draw `max_features` features.
                        - If float, then draw `max_features * X.shape[1]` features.
   :type max_features: int or float, default=1.0
   :param bootstrap: Whether samples are drawn with replacement. If False, sampling
                     without replacement is performed.
   :type bootstrap: bool, default=True
   :param bootstrap_features: Whether features are drawn with replacement.
   :type bootstrap_features: bool, default=False
   :param oob_score: Whether to use out-of-bag samples to estimate
                     the generalization error. Only available if bootstrap=True.
   :type oob_score: bool, default=False
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble. See :term:`the Glossary <warm_start>`.

                      .. versionadded:: 0.17
                         *warm_start* constructor parameter.
   :type warm_start: bool, default=False
   :param n_jobs: The number of jobs to run in parallel for both :meth:`fit` and
                  :meth:`predict`. ``None`` means 1 unless in a
                  :obj:`joblib.parallel_backend` context. ``-1`` means using all
                  processors. See :term:`Glossary <n_jobs>` for more details.
   :type n_jobs: int, default=None
   :param random_state: Controls the random resampling of the original dataset
                        (sample wise and feature wise).
                        If the base estimator accepts a `random_state` attribute, a different
                        seed is generated for each instance in the ensemble.
                        Pass an int for reproducible output across multiple function calls.
                        See :term:`Glossary <random_state>`.
   :type random_state: int, RandomState instance or None, default=None
   :param verbose: Controls the verbosity when fitting and predicting.
   :type verbose: int, default=0

   .. attribute:: base_estimator_

      The base estimator from which the ensemble is grown.

      :type: estimator

   .. attribute:: n_features_

      The number of features when :meth:`fit` is performed.

      .. deprecated:: 1.0
          Attribute `n_features_` was deprecated in version 1.0 and will be
          removed in 1.2. Use `n_features_in_` instead.

      :type: int

   .. attribute:: n_features_in_

      Number of features seen during :term:`fit`.

      .. versionadded:: 0.24

      :type: int

   .. attribute:: feature_names_in_

      Names of features seen during :term:`fit`. Defined only when `X`
      has feature names that are all strings.

      .. versionadded:: 1.0

      :type: ndarray of shape (`n_features_in_`,)

   .. attribute:: estimators_

      The collection of fitted base estimators.

      :type: list of estimators

   .. attribute:: estimators_samples_

      The subset of drawn samples (i.e., the in-bag samples) for each base
      estimator. Each subset is defined by an array of the indices selected.

      :type: list of arrays

   .. attribute:: estimators_features_

      The subset of drawn features for each base estimator.

      :type: list of arrays

   .. attribute:: classes_

      The classes labels.

      :type: ndarray of shape (n_classes,)

   .. attribute:: n_classes_

      The number of classes.

      :type: int or list

   .. attribute:: oob_score_

      Score of the training dataset obtained using an out-of-bag estimate.
      This attribute exists only when ``oob_score`` is True.

      :type: float

   .. attribute:: oob_decision_function_

      Decision function computed with out-of-bag estimate on the training
      set. If n_estimators is small it might be possible that a data point
      was never left out during the bootstrap. In this case,
      `oob_decision_function_` might contain NaN. This attribute exists
      only when ``oob_score`` is True.

      :type: ndarray of shape (n_samples, n_classes)

   .. seealso::

      :obj:`BaggingRegressor`
          A Bagging regressor.

   .. rubric:: References

   .. [1] L. Breiman, "Pasting small votes for classification in large
          databases and on-line", Machine Learning, 36(1), 85-103, 1999.

   .. [2] L. Breiman, "Bagging predictors", Machine Learning, 24(2), 123-140,
          1996.

   .. [3] T. Ho, "The random subspace method for constructing decision
          forests", Pattern Analysis and Machine Intelligence, 20(8), 832-844,
          1998.

   .. [4] G. Louppe and P. Geurts, "Ensembles on Random Patches", Machine
          Learning and Knowledge Discovery in Databases, 346-361, 2012.

   .. rubric:: Examples

   >>> from sklearn.svm import SVC
   >>> from sklearn.ensemble import BaggingClassifier
   >>> from sklearn.datasets import make_classification
   >>> X, y = make_classification(n_samples=100, n_features=4,
   ...                            n_informative=2, n_redundant=0,
   ...                            random_state=0, shuffle=False)
   >>> clf = BaggingClassifier(base_estimator=SVC(),
   ...                         n_estimators=10, random_state=0).fit(X, y)
   >>> clf.predict([[0, 0, 0, 0]])
   array([1])


.. py:class:: RockestRegressor(n_estimators=100, *, n_kernels=10, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, sampling='auto', sampling_params=None, kernel_size=None, bias_prob=1.0, normalize_prob=1.0, padding_prob=0.5, criterion='mse', bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseForestRegressor`

   A Bagging regressor.

   A Bagging regressor is an ensemble meta-estimator that fits base
   regressors each on random subsets of the original dataset and then
   aggregate their individual predictions (either by voting or by averaging)
   to form a final prediction. Such a meta-estimator can typically be used as
   a way to reduce the variance of a black-box estimator (e.g., a decision
   tree), by introducing randomization into its construction procedure and
   then making an ensemble out of it.

   This algorithm encompasses several works from the literature. When random
   subsets of the dataset are drawn as random subsets of the samples, then
   this algorithm is known as Pasting [1]_. If samples are drawn with
   replacement, then the method is known as Bagging [2]_. When random subsets
   of the dataset are drawn as random subsets of the features, then the method
   is known as Random Subspaces [3]_. Finally, when base estimators are built
   on subsets of both samples and features, then the method is known as
   Random Patches [4]_.

   Read more in the :ref:`User Guide <bagging>`.

   .. versionadded:: 0.15

   :param base_estimator: The base estimator to fit on random subsets of the dataset.
                          If None, then the base estimator is a
                          :class:`~sklearn.tree.DecisionTreeRegressor`.
   :type base_estimator: object, default=None
   :param n_estimators: The number of base estimators in the ensemble.
   :type n_estimators: int, default=10
   :param max_samples: The number of samples to draw from X to train each base estimator (with
                       replacement by default, see `bootstrap` for more details).

                       - If int, then draw `max_samples` samples.
                       - If float, then draw `max_samples * X.shape[0]` samples.
   :type max_samples: int or float, default=1.0
   :param max_features: The number of features to draw from X to train each base estimator (
                        without replacement by default, see `bootstrap_features` for more
                        details).

                        - If int, then draw `max_features` features.
                        - If float, then draw `max_features * X.shape[1]` features.
   :type max_features: int or float, default=1.0
   :param bootstrap: Whether samples are drawn with replacement. If False, sampling
                     without replacement is performed.
   :type bootstrap: bool, default=True
   :param bootstrap_features: Whether features are drawn with replacement.
   :type bootstrap_features: bool, default=False
   :param oob_score: Whether to use out-of-bag samples to estimate
                     the generalization error. Only available if bootstrap=True.
   :type oob_score: bool, default=False
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble. See :term:`the Glossary <warm_start>`.
   :type warm_start: bool, default=False
   :param n_jobs: The number of jobs to run in parallel for both :meth:`fit` and
                  :meth:`predict`. ``None`` means 1 unless in a
                  :obj:`joblib.parallel_backend` context. ``-1`` means using all
                  processors. See :term:`Glossary <n_jobs>` for more details.
   :type n_jobs: int, default=None
   :param random_state: Controls the random resampling of the original dataset
                        (sample wise and feature wise).
                        If the base estimator accepts a `random_state` attribute, a different
                        seed is generated for each instance in the ensemble.
                        Pass an int for reproducible output across multiple function calls.
                        See :term:`Glossary <random_state>`.
   :type random_state: int, RandomState instance or None, default=None
   :param verbose: Controls the verbosity when fitting and predicting.
   :type verbose: int, default=0

   .. attribute:: base_estimator_

      The base estimator from which the ensemble is grown.

      :type: estimator

   .. attribute:: n_features_

      The number of features when :meth:`fit` is performed.

      .. deprecated:: 1.0
          Attribute `n_features_` was deprecated in version 1.0 and will be
          removed in 1.2. Use `n_features_in_` instead.

      :type: int

   .. attribute:: n_features_in_

      Number of features seen during :term:`fit`.

      .. versionadded:: 0.24

      :type: int

   .. attribute:: feature_names_in_

      Names of features seen during :term:`fit`. Defined only when `X`
      has feature names that are all strings.

      .. versionadded:: 1.0

      :type: ndarray of shape (`n_features_in_`,)

   .. attribute:: estimators_

      The collection of fitted sub-estimators.

      :type: list of estimators

   .. attribute:: estimators_samples_

      The subset of drawn samples (i.e., the in-bag samples) for each base
      estimator. Each subset is defined by an array of the indices selected.

      :type: list of arrays

   .. attribute:: estimators_features_

      The subset of drawn features for each base estimator.

      :type: list of arrays

   .. attribute:: oob_score_

      Score of the training dataset obtained using an out-of-bag estimate.
      This attribute exists only when ``oob_score`` is True.

      :type: float

   .. attribute:: oob_prediction_

      Prediction computed with out-of-bag estimate on the training
      set. If n_estimators is small it might be possible that a data point
      was never left out during the bootstrap. In this case,
      `oob_prediction_` might contain NaN. This attribute exists only
      when ``oob_score`` is True.

      :type: ndarray of shape (n_samples,)

   .. seealso::

      :obj:`BaggingClassifier`
          A Bagging classifier.

   .. rubric:: References

   .. [1] L. Breiman, "Pasting small votes for classification in large
          databases and on-line", Machine Learning, 36(1), 85-103, 1999.

   .. [2] L. Breiman, "Bagging predictors", Machine Learning, 24(2), 123-140,
          1996.

   .. [3] T. Ho, "The random subspace method for constructing decision
          forests", Pattern Analysis and Machine Intelligence, 20(8), 832-844,
          1998.

   .. [4] G. Louppe and P. Geurts, "Ensembles on Random Patches", Machine
          Learning and Knowledge Discovery in Databases, 346-361, 2012.

   .. rubric:: Examples

   >>> from sklearn.svm import SVR
   >>> from sklearn.ensemble import BaggingRegressor
   >>> from sklearn.datasets import make_regression
   >>> X, y = make_regression(n_samples=100, n_features=4,
   ...                        n_informative=2, n_targets=1,
   ...                        random_state=0, shuffle=False)
   >>> regr = BaggingRegressor(base_estimator=SVR(),
   ...                         n_estimators=10, random_state=0).fit(X, y)
   >>> regr.predict([[0, 0, 0, 0]])
   array([-2.8720...])


.. py:class:: ShapeletForestClassifier(n_estimators=100, *, n_shapelets=10, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, metric='euclidean', metric_params=None, criterion='entropy', oob_score=False, bootstrap=True, warm_start=False, class_weight=None, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestClassifier`

   An ensemble of random shapelet tree classifiers.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ShapeletForestClassifier
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ShapeletForestClassifier(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Shapelet forest classifier.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param n_shapelets: The number of shapelets to sample at each node
   :type n_shapelets: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param min_samples_leaf: The minimum number of samples in a leaf
   :type min_samples_leaf: int, optional
   :param criterion: The criterion used to evaluate the utility of a split
   :type criterion: {"entropy", "gini"}, optional
   :param min_impurity_decrease: A split will be introduced only if the impurity decrease is larger than or
                                 equal to this value
   :type min_impurity_decrease: float, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param oob_score: Compute out-of-bag estimates of the ensembles performance.
   :type oob_score: bool, optional
   :param class_weight: Weights associated with the labels

                        - if dict, weights on the form {label: weight}
                        - if "balanced" each class weight inversely proportional to the class
                          frequency
                        - if None, each class has equal weight
   :type class_weight: dict or "balanced", optional
   :param random_state: Controls the random resampling of the original dataset and the construction
                        of the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional


.. py:class:: ShapeletForestEmbedding(n_estimators=100, *, n_shapelets=1, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, metric='euclidean', metric_params=None, criterion='mse', bootstrap=True, warm_start=False, n_jobs=None, sparse_output=True, random_state=None)

   Bases: :py:obj:`BaseShapeletForestRegressor`

   An ensemble of random shapelet trees

   An unsupervised transformation of a time series dataset
   to a high-dimensional sparse representation. A time series i
   indexed by the leaf that it falls into. This leads to a binary
   coding of a time series with as many ones as trees in the forest.

   The dimensionality of the resulting representation is
   ``<= n_estimators * 2^max_depth``

   Construct a shapelet forest embedding.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param sparse_output: Return a sparse CSR-matrix.
   :type sparse_output: bool, optional
   :param random_state: Controls the random resampling of the original dataset and the construction
                        of the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional

   .. py:method:: fit(self, x, y=None, sample_weight=None, check_input=True)

      Build a Bagging ensemble of estimators from the training set (X, y).

      :param X: The training input samples. Sparse matrices are accepted only if
                they are supported by the base estimator.
      :type X: {array-like, sparse matrix} of shape (n_samples, n_features)
      :param y: The target values (class labels in classification, real numbers in
                regression).
      :type y: array-like of shape (n_samples,)
      :param sample_weight: Sample weights. If None, then samples are equally weighted.
                            Note that this is supported only if the base estimator supports
                            sample weighting.
      :type sample_weight: array-like of shape (n_samples,), default=None

      :returns: **self** -- Fitted estimator.
      :rtype: object


   .. py:method:: fit_transform(self, x, y=None, sample_weight=None, check_input=True)


   .. py:method:: transform(self, x)



.. py:class:: ShapeletForestRegressor(n_estimators=100, *, n_shapelets=10, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, metric='euclidean', metric_params=None, criterion='mse', oob_score=False, bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestRegressor`

   An ensemble of random shapelet regression trees.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ShapeletForestRegressor
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ShapeletForestRegressor(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Shapelet forest regressor.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param n_shapelets: The number of shapelets to sample at each node
   :type n_shapelets: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param oob_score: Compute out-of-bag estimates of the ensembles performance.
   :type oob_score: bool, optional
   :param random_state: Controls the random resampling of the original dataset and the construction
                        of the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional


