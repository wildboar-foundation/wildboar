
****************************
:py:mod:`wildboar.transform`
****************************

.. py:module:: wildboar.transform

.. autoapi-nested-parse::

   
   Transform raw time series to tabular representations.
















   ..
       !! processed by numpydoc !!


Package Contents
----------------

Classes
-------

.. autoapisummary::

   wildboar.transform.FeatureTransform
   wildboar.transform.IntervalTransform
   wildboar.transform.MatrixProfileTransform
   wildboar.transform.PAA
   wildboar.transform.PivotTransform
   wildboar.transform.ProximityTransform
   wildboar.transform.RandomShapeletTransform
   wildboar.transform.RocketTransform
   wildboar.transform.SAX



Functions
---------

.. autoapisummary::

   wildboar.transform.piecewice_aggregate_approximation
   wildboar.transform.symbolic_aggregate_approximation



.. py:class:: FeatureTransform(*, summarizer='catch22', n_jobs=None)


   Bases: :py:obj:`IntervalTransform`

   
   Transform a time series as a number of features.


   :Parameters:

       **summarizer** : str or list, optional
           The method to summarize each interval.
           
           - if str, the summarizer is determined by `_SUMMARIZERS.keys()`.
           - if list, the summarizer is a list of functions f(x) -> float, where x
             is a numpy array.
           
           The default summarizer summarizes each time series using `catch22`-features.

       **n_jobs** : int, optional
           The number of cores to use on multi-core.














   ..
       !! processed by numpydoc !!

.. py:class:: IntervalTransform(n_intervals='sqrt', *, intervals='fixed', sample_size=0.5, min_size=0.0, max_size=1.0, summarizer='mean_var_slope', n_jobs=None, random_state=None)


   Bases: :py:obj:`IntervalMixin`, :py:obj:`wildboar.transform._base.BaseFeatureEngineerTransform`

   
   Embed a time series as a collection of features per interval.


   :Parameters:

       **n_intervals** : str, int or float, optional
           The number of intervals to use for the transform.
           
           - if "log2", the number of intervals is `log2(n_timestep)`.
           - if "sqrt", the number of intervals is `sqrt(n_timestep)`.
           - if int, the number of intervals is `n_intervals`.
           - if float, the number of intervals is `n_intervals * n_timestep`, with
               `0 < n_intervals < 1`.

       **intervals** : str, optional
           The method for selecting intervals
           
           - if "fixed", `n_intervals` non-overlapping intervals.
           - if "sample", `n_intervals * sample_size` non-overlapping intervals.
           - if "random", `n_intervals` possibly overlapping intervals of randomly
               sampled in `[min_size * n_timestep, max_size * n_timestep]`.

       **sample_size** : float, optional
           The sample size of fixed intervals if `intervals="sample"`.

       **min_size** : float, optional
           The minimum interval size if `intervals="random"`.

       **max_size** : float, optional
           The maximum interval size if `intervals="random"`.

       **summarizer** : str or list, optional
           The method to summarize each interval.
           
           - if str, the summarizer is determined by `_SUMMARIZERS.keys()`.
           - if list, the summarizer is a list of functions `f(x) -> float`, where
               x is a numpy array.
           
           The default summarizer summarizes each interval as its mean, standard
           deviation and slope.

       **n_jobs** : int, optional
           The number of cores to use on multi-core.

       **random_state** : int or RandomState
           - If `int`, `random_state` is the seed used by the random number generator
           - If `RandomState` instance, `random_state` is the random number generator
           - If `None`, the random number generator is the `RandomState` instance used
               by `np.random`.










   .. rubric:: References

   Lubba, Carl H., Sarab S. Sethi, Philip Knaute, Simon R. Schultz,             Ben D. Fulcher, and Nick S. Jones.
       catch22: Canonical time-series characteristics.
       Data Mining and Knowledge Discovery 33, no. 6 (2019): 1821-1852.

   .. only:: latex

      

   .. rubric:: Examples

   >>> from wildboar.datasets import load_dataset
   >>> x, y = load_dataset("GunPoint")
   >>> t = IntervalTransform(n_intervals=10, summarizer="mean")
   >>> t.fit_transform(x)

   Each interval (15 timepoints) are transformed to their mean.

   >>> t = IntervalTransform(n_intervals="sqrt", summarizer=[np.mean, np.std])
   >>> t.fit_transform(x)

   Each interval (150 // 12 timepoints) are transformed to two features. The mean
   and the standard deviation.



   ..
       !! processed by numpydoc !!

.. py:class:: MatrixProfileTransform(window=0.1, exclude=None, n_jobs=None)


   Bases: :py:obj:`sklearn.base.TransformerMixin`, :py:obj:`wildboar.base.BaseEstimator`

   
   Matrix profile transform.

   Transform each time series in a dataset to its MatrixProfile similarity
   self-join.

   :Parameters:

       **window** : int or float, optional
           The subsequence size, by default 0.1.
           
           - if float, a fraction of n_timestep.
           - if int, the exact subsequence size.

       **exclude** : int or float, optional
           The size of the exclusion zone. The default exclusion zone is 0.2.
           
           - if float, expressed as a fraction of the windows size.
           - if int, exact size (0 < exclude).

       **n_jobs** : int, optional
           The number of jobs to use when computing the profile.











   .. rubric:: Examples

   >>> from wildboar.datasets import load_two_lead_ecg()
   >>> from wildboar.transform import MatrixProfileTransform
   >>> x, y = load_two_lead_ecg()
   >>> t = MatrixProfileTransform()
   >>> t.fit_transform(x)



   ..
       !! processed by numpydoc !!
   .. py:method:: fit(x, y=None)

      
      Fit the matrix profile.

      Sets the expected input dimensions.

      :Parameters:

          **x** : array-like of shape (n_samples, n_timesteps)         or (n_samples, n_dims, n_timesteps)
              The samples.

          **y** : ignored
              The optional labels.

      :Returns:

          self
              A fitted instance.













      ..
          !! processed by numpydoc !!

   .. py:method:: transform(x)

      
      Transform the samples to their MatrixProfile self-join.


      :Parameters:

          **x** : array-like of shape (n_samples, n_timesteps)         or (n_samples, n_dims, n_timesteps)
              The samples.

      :Returns:

          ndarray of shape (n_samples, n_timestep) or (n_samples, n_dims, n_timesteps)
              The matrix matrix profile of each sample.













      ..
          !! processed by numpydoc !!


.. py:class:: PAA(n_intervals='sqrt', window=None)


   Bases: :py:obj:`sklearn.base.TransformerMixin`, :py:obj:`wildboar.base.BaseEstimator`

   
   Peicewise aggregate approximation.


   :Parameters:

       **n_intervals** : {"sqrt", "log2"}, int or float, optional
           The number of intervals.

       **window** : int, optional
           The size of an interval. If `window`, is given then `n_intervals` is ignored.














   ..
       !! processed by numpydoc !!
   .. py:property:: intervals


   .. py:method:: fit(x, y=None)


   .. py:method:: inverse_transform(x)


   .. py:method:: transform(x)



.. py:class:: PivotTransform(n_pivots=100, *, metric='auto', metric_params=None, metric_sample=None, random_state=None, n_jobs=None)


   Bases: :py:obj:`PivotMixin`, :py:obj:`wildboar.transform._base.BaseFeatureEngineerTransform`

   
   A transform using pivot time series and sampled distance metrics.


   :Parameters:

       **n_pivots** : int, optional
           The number of pivot time series.

       **metric** : {'auto'} or list, optional
           - If str, the metric to compute the distance.
           - If list, multiple metrics specified as a list of tuples, where the first
               element of the tuple is a metric name and the second element a dictionary
               with a parameter grid specification. A parameter grid specification is a
               dict with two mandatory and one optional key-value pairs defining the
               lower and upper bound on the values and number of values in the grid. For
               example, to specifiy a grid over the argument 'r' with 10 values in the
               range 0 to 1, we would give the following specification: `dict(min_r=0,
               max_r=1, num_r=10)`.
           
           Read more about the metrics and their parameters in the :ref:`User guide
           <list_of_subsequence_metrics>`.

       **metric_params** : dict, optional
           Parameters for the distance measure. Ignored unless metric is a string.
           
           Read more about the parameters in the :ref:`User guide <list_of_metrics>`.

       **metric_sample** : {"uniform", "weighted"}, optional
           If multiple metrics are specified this parameter controls how they are
           sampled. "uniform" samples each metric configuration with equal probability
           and "weighted" samples each metric with equal probability. By default,
           metric configurations are sampled with equal probability.

       **random_state** : int or np.RandomState, optional
           The random state.

       **n_jobs** : int, optional
           The number of cores to use.














   ..
       !! processed by numpydoc !!

.. py:class:: ProximityTransform(n_pivots=100, metric='auto', metric_params=None, metric_sample='weighted', random_state=None, n_jobs=None)


   Bases: :py:obj:`sklearn.base.TransformerMixin`, :py:obj:`wildboar.base.BaseEstimator`

   
   Transform time series based on class conditional pivots.


   :Parameters:

       **n_pivots** : int, optional
           The number of pivot time series per class.

       **metric** : {'auto'} or list, optional
           - If str, the metric to compute the distance.
           - If list, multiple metrics specified as a list of tuples, where the first
               element of the tuple is a metric name and the second element a dictionary
               with a parameter grid specification. A parameter grid specification is a
               dict with two mandatory and one optional key-value pairs defining the
               lower and upper bound on the values and number of values in the grid. For
               example, to specifiy a grid over the argument 'r' with 10 values in the
               range 0 to 1, we would give the following specification: `dict(min_r=0,
               max_r=1, num_r=10)`.
           
           Read more about the metrics and their parameters in the :ref:`User guide
           <list_of_subsequence_metrics>`.

       **metric_params** : dict, optional
           Parameters for the distance measure. Ignored unless metric is a string.
           
           Read more about the parameters in the :ref:`User guide <list_of_metrics>`.

       **metric_sample** : {"uniform", "weighted"}, optional
           If multiple metrics are specified this parameter controls how they are
           sampled. "uniform" samples each metric configuration with equal probability
           and "weighted" samples each metric with equal probability. By default,
           metric configurations are sampled with equal probability.

       **random_state** : int or np.RandomState, optional
           The random state.

       **n_jobs** : int, optional
           The number of cores to use.














   ..
       !! processed by numpydoc !!
   .. py:method:: fit(X, y)


   .. py:method:: transform(X, y=None)



.. py:class:: RandomShapeletTransform(n_shapelets=1000, *, metric='euclidean', metric_params=None, min_shapelet_size=0.0, max_shapelet_size=1.0, n_jobs=None, random_state=None)


   Bases: :py:obj:`ShapeletMixin`, :py:obj:`wildboar.transform._base.BaseFeatureEngineerTransform`

   
   Random shapelet tranform.

   Transform a time series to the distances to a selection of random
   shapelets.

   :Parameters:

       **n_shapelets** : int, optional
           The number of shapelets in the resulting transform.

       **metric** : str or list, optional
           - If str, the distance metric used to identify the best shapelet.
           - If list, multiple metrics specified as a list of tuples, where the first
               element of the tuple is a metric name and the second element a dictionary
               with a parameter grid specification. A parameter grid specification is a
               dict with two mandatory and one optional key-value pairs defining the
               lower and upper bound on the values and number of values in the grid. For
               example, to specifiy a grid over the argument 'r' with 10 values in the
               range 0 to 1, we would give the following specification: ``dict(min_r=0,
               max_r=1, num_r=10)``.
           
           Read more about the metrics and their parameters in the
           :ref:`User guide <list_of_subsequence_metrics>`.

       **metric_params** : dict, optional
           Parameters for the distance measure. Ignored unless metric is a string.
           
           Read more about the parameters in the :ref:`User guide
           <list_of_subsequence_metrics>`.

       **min_shapelet_size** : float, optional
           Minimum shapelet size.

       **max_shapelet_size** : float, optional
           Maximum shapelet size.

       **n_jobs** : int, optional
           The number of jobs to run in parallel. None means 1 and -1 means using all
           processors.

       **random_state** : int or RandomState
           - If `int`, `random_state` is the seed used by the random number generator
           - If `RandomState` instance, `random_state` is the random number generator
           - If `None`, the random number generator is the `RandomState` instance used
               by `np.random`.










   .. rubric:: References

   Wistuba, Martin, Josif Grabocka, and Lars Schmidt-Thieme.
       Ultra-fast shapelets for time series classification. arXiv preprint
       arXiv:1503.05018 (2015).

   .. only:: latex

      

   .. rubric:: Examples

   Transform each time series to the minimum DTW distance to each shapelet

   >>> from wildboar.dataset import load_gunpoint()
   >>> from wildboar.transform import RandomShapeletTransform
   >>> t = RandomShapeletTransform(metric="dtw")
   >>> t.fit_transform(X)

   Transform each time series to the either the minimum DTW distance, with r randomly
   set set between 0 and 1 or ERP distance with g between 0 and 1.

   >>> t = RandomShapeletTransform(
   ...     metric=[
   ...         ("dtw", dict(min_r=0.0, max_r=1.0)),
   ...         ("erp", dict(min_g=0.0, max_g=1.0)),
   ...     ]
   ... )
   >>> t.fit_transform(X)

   :Attributes:

       **embedding_** : Embedding
           The underlying embedding object.


   ..
       !! processed by numpydoc !!

.. py:class:: RocketTransform(n_kernels=1000, *, sampling='normal', sampling_params=None, kernel_size=None, min_size=None, max_size=None, bias_prob=1.0, normalize_prob=1.0, padding_prob=0.5, n_jobs=None, random_state=None)


   Bases: :py:obj:`RocketMixin`, :py:obj:`wildboar.transform._base.BaseFeatureEngineerTransform`

   
   Transform a time series using random convolution features.


   :Parameters:

       **n_kernels** : int, optional
           The number of kernels to sample at each node.

       **sampling** : {"normal", "uniform", "shapelet"}, optional
           The sampling of convolutional filters.
           
           - if "normal", sample filter according to a normal distribution with
             ``mean`` and ``scale``.
           - if "uniform", sample filter according to a uniform distribution with
             ``lower`` and ``upper``.
           - if "shapelet", sample filters as subsequences in the training data.

       **sampling_params** : dict, optional
           Parameters for the sampling strategy.
           
           - if "normal", ``{"mean": float, "scale": float}``, defaults to
             ``{"mean": 0, "scale": 1}``.
           - if "uniform", ``{"lower": float, "upper": float}``, defaults to
             ``{"lower": -1, "upper": 1}``.

       **kernel_size** : array-like, optional
           The kernel size, by default ``[7, 11, 13]``.

       **min_size** : float, optional
           The minimum timestep size used for generating kernel sizes, If set,
           ``kernel_size`` is ignored.

       **max_size** : float, optional
           The maximum timestep size used for generating kernel sizes, If set,
           ``kernel_size`` is ignored.

       **bias_prob** : float, optional
           The probability of using the bias term.

       **normalize_prob** : float, optional
           The probability of performing normalization.

       **padding_prob** : float, optional
           The probability of padding with zeros.

       **n_jobs** : int, optional
           The number of jobs to run in parallel. A value of ``None`` means using
           a single core and a value of ``-1`` means using all cores. Positive
           integers mean the exact number of cores.

       **random_state** : int or RandomState, optional
           Controls the random resampling of the original dataset.
           
           - If ``int``, ``random_state`` is the seed used by the random number
             generator.
           - If :class:`numpy.random.RandomState` instance, ``random_state`` is
             the random number generator.
           - If ``None``, the random number generator is the
             :class:`numpy.random.RandomState` instance used by
             :func:`numpy.random`.










   .. rubric:: References

   Dempster, Angus, François Petitjean, and Geoffrey I. Webb.
       ROCKET: exceptionally fast and accurate time series classification using
       random convolutional kernels.
       Data Mining and Knowledge Discovery 34.5 (2020): 1454-1495.

   .. only:: latex

      


   :Attributes:

       **embedding_** : Embedding
           The underlying embedding


   ..
       !! processed by numpydoc !!

.. py:class:: SAX(*, n_intervals='sqrt', window=None, n_bins=4, binning='normal', estimate=True)


   Bases: :py:obj:`sklearn.base.TransformerMixin`, :py:obj:`wildboar.base.BaseEstimator`

   
   Symbolic aggregate approximation.


   :Parameters:

       **n_intervals** : str, optional
           The number of intervals to use for the transform.
           
           - if "log", the number of intervals is `log2(n_timestep)`.
           - if "sqrt", the number of intervals is `sqrt(n_timestep)`.
           - if int, the number of intervals is `n_intervals`.
           - if float, the number of intervals is `n_intervals * n_timestep`, with
               `0 < n_intervals < 1`.

       **window** : int, optional
           The window size. If `window` is set, the value of `n_intervals` has no
           effect.

       **n_bins** : int, optional
           The number of bins.

       **binning** : str, optional
           The bin construction. By default the bins are defined according to the
           normal distribution. Possible values are "normal" for normally
           distributed bins or "uniform" for uniformly distributed bins.

       **estimate** : bool, optional
           Estimate the distribution parameters for the binning from data.
           
           If `estimate=False`, it is assumed that each time series are:
           
           - preprocessed using :func:`datasets.preprocess.normalize` when
               `binning="normal"`.
           - preprocessed using :func:`datasets.preprocess.minmax_scale`. when
               `binning="uniform"`














   ..
       !! processed by numpydoc !!
   .. py:property:: intervals


   .. py:method:: fit(x, y=None)


   .. py:method:: inverse_transform(x)


   .. py:method:: transform(x)



.. py:function:: piecewice_aggregate_approximation(x, *, n_intervals='sqrt', window=None)

   
   Peicewise aggregate approximation.


   :Parameters:

       **x** : array-like of shape (n_samples, n_timestep)
           The input data.

       **n_intervals** : str, optional
           The number of intervals to use for the transform.
           
           - if "log2", the number of intervals is ``log2(n_timestep)``.
           - if "sqrt", the number of intervals is ``sqrt(n_timestep)``.
           - if int, the number of intervals is ``n_intervals``.
           - if float, the number of intervals is ``n_intervals * n_timestep``, with
               ``0 < n_intervals < 1``.

       **window** : int, optional
           The window size. If ``window`` is set, the value of ``n_intervals`` has no
           effect.

   :Returns:

       ndarray of shape (n_samples, n_intervals)
           The symbolic aggregate approximation.













   ..
       !! processed by numpydoc !!

.. py:function:: symbolic_aggregate_approximation(x, *, n_intervals='sqrt', window=None, n_bins=4, binning='normal')

   
   Symbolic aggregate approximation.


   :Parameters:

       **x** : array-like of shape (n_samples, n_timestep)
           The input data.

       **n_intervals** : str, optional
           The number of intervals to use for the transform.
           
           - if "log2", the number of intervals is ``log2(n_timestep)``.
           - if "sqrt", the number of intervals is ``sqrt(n_timestep)``.
           - if int, the number of intervals is ``n_intervals``.
           - if float, the number of intervals is ``n_intervals * n_timestep``, with
               ``0 < n_intervals < 1``.

       **window** : int, optional
           The window size. If ``window`` is set, the value of ``n_intervals`` has no
           effect.

       **n_bins** : int, optional
           The number of bins.

       **binning** : str, optional
           The bin construction. By default the bins are defined according to the
           normal distribution. Possible values are ``"normal"`` for normally
           distributed bins or ``"uniform"`` for uniformly distributed bins.

   :Returns:

       ndarray of shape (n_samples, n_intervals)
           The symbolic aggregate approximation.













   ..
       !! processed by numpydoc !!

