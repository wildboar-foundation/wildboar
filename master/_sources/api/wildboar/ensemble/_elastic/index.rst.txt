:orphan:


************************************
:py:mod:`wildboar.ensemble._elastic`
************************************

.. py:module:: wildboar.ensemble._elastic



Classes
-------

.. autoapisummary::

   wildboar.ensemble._elastic.ElasticEnsembleClassifier




.. py:class:: ElasticEnsembleClassifier(n_neighbors=1, *, metric='auto', n_jobs=None)




   
   Ensemble of :class:`wildboar.distance.KNeighborsClassifier`.

   Each classifier is fitted with an optimized parameter grid
   over metric parameters.

   :Parameters:

       **n_neighbors** : int, optional
           The number of neighbors.

       **metric** : {"auto", "elastic", "non_elastic", "all"} or dict, optional
           The metric specification.
           
           - if "auto" or "elastic", fit one classifier for each elastic distance
             as described by Lines and Bagnall (2015). We use a slightly smaller
             parameter grid.
           - if "non_elastic", fit one classifier for each non-elastic distance
             measure.
           - if "all", fit one classifier for the metrics in both "elastic" and
             "non_elastic".
           - if dict, a custom metric specification.

       **n_jobs** : int, optional
           The number of paralell jobs.










   .. rubric:: References

   Jason Lines and Anthony Bagnall,
       Time Series Classification with Ensembles of Elastic Distance Measures,
       Data Mining and Knowledge Discovery, 29(3), 2015.

   .. only:: latex

      

   .. rubric:: Examples

   >>> from wildboar.datasets import load_gun_point
   >>> from wildboar.ensemble import ElasticEnsembleClassifier
   >>> X_train, X_test, y_train, y_test = load_gun_point(merge_train_test=False)
   >>> clf = ElasticEnsembleClassifier(
   ...     metric={
   ...         "dtw": {"min_r": 0.1, "max_r": 0.3},
   ...         "ddtw": {"min_r": 0.1, "max_r": 0.3},
   ...     },
   ... )
   >>> clf.fit(X_train, y_train)
   ElasticEnsembleClassifier(metric={'ddtw': {'max_r': 0.3, 'min_r': 0.1},
                                     'dtw': {'max_r': 0.3, 'min_r': 0.1}})
   >>> clf.score(X_test, y_test)
   0.9866666666666667

   :Attributes:

       **scores** : tuple
           A tuple of metric name and cross-validation score.


   ..
       !! processed by numpydoc !!
   .. py:method:: fit(x, y)

      
      Fit the estimator.


      :Parameters:

          **x** : array-like of shape (n_samples, n_timesteps) or (n_samples, n_dim, n_timesteps)
              The input samples.

          **y** : array-like of shape (n_samples, )
              The input labels.

      :Returns:

          object
              This estimator.













      ..
          !! processed by numpydoc !!

   .. py:method:: get_metadata_routing()

      
      Get metadata routing of this object.

      Please check :ref:`User Guide <metadata_routing>` on how the routing
      mechanism works.


      :Returns:

          **routing** : MetadataRequest
              A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
              routing information.













      ..
          !! processed by numpydoc !!

   .. py:method:: get_params(deep=True)

      
      Get parameters for this estimator.


      :Parameters:

          **deep** : bool, default=True
              If True, will return the parameters for this estimator and
              contained subobjects that are estimators.

      :Returns:

          **params** : dict
              Parameter names mapped to their values.













      ..
          !! processed by numpydoc !!

   .. py:method:: predict(x)

      
      Compute the class label for the samples in x.


      :Parameters:

          **x** : array-like of shape (n_samples, n_timesteps) or (n_samples, n_dim, n_timesteps)
              The input samples.

      :Returns:

          ndarray of shape (n_samples, )
              The class label for each sample.













      ..
          !! processed by numpydoc !!

   .. py:method:: predict_proba(x)

      
      Compute probability estimates for the samples in x.


      :Parameters:

          **x** : array-like of shape (n_samples, n_timesteps) or (n_samples, n_dim, n_timesteps)
              The input time series.

      :Returns:

          ndarray of shape (n_samples, n_classes)
              The probabilities.













      ..
          !! processed by numpydoc !!

   .. py:method:: score(X, y, sample_weight=None)

      
      Return the mean accuracy on the given test data and labels.

      In multi-label classification, this is the subset accuracy
      which is a harsh metric since you require for each sample that
      each label set be correctly predicted.

      :Parameters:

          **X** : array-like of shape (n_samples, n_features)
              Test samples.

          **y** : array-like of shape (n_samples,) or (n_samples, n_outputs)
              True labels for `X`.

          **sample_weight** : array-like of shape (n_samples,), default=None
              Sample weights.

      :Returns:

          **score** : float
              Mean accuracy of ``self.predict(X)`` w.r.t. `y`.













      ..
          !! processed by numpydoc !!

   .. py:method:: set_params(**params)

      
      Set the parameters of this estimator.

      The method works on simple estimators as well as on nested objects
      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
      parameters of the form ``<component>__<parameter>`` so that it's
      possible to update each component of a nested object.

      :Parameters:

          **\*\*params** : dict
              Estimator parameters.

      :Returns:

          **self** : estimator instance
              Estimator instance.













      ..
          !! processed by numpydoc !!


