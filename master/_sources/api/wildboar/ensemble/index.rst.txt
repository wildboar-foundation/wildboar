
***************************
:py:mod:`wildboar.ensemble`
***************************

.. py:module:: wildboar.ensemble

.. autoapi-nested-parse::

   
   Ensemble methods for classification, regression and outlier detection.
















   ..
       !! processed by numpydoc !!


Package Contents
----------------

Classes
-------

.. autoapisummary::

   wildboar.ensemble.BaggingClassifier
   wildboar.ensemble.BaggingRegressor
   wildboar.ensemble.BaseBagging
   wildboar.ensemble.ExtraShapeletTreesClassifier
   wildboar.ensemble.ExtraShapeletTreesRegressor
   wildboar.ensemble.IntervalForestClassifier
   wildboar.ensemble.IntervalForestRegressor
   wildboar.ensemble.IsolationShapeletForest
   wildboar.ensemble.PivotForestClassifier
   wildboar.ensemble.ProximityForestClassifier
   wildboar.ensemble.RocketForestClassifier
   wildboar.ensemble.RocketForestRegressor
   wildboar.ensemble.ShapeletForestClassifier
   wildboar.ensemble.ShapeletForestEmbedding
   wildboar.ensemble.ShapeletForestRegressor




.. py:class:: BaggingClassifier(estimator=None, n_estimators=10, *, max_samples=1.0, bootstrap=True, oob_score=False, class_weight=None, warm_start=False, n_jobs=None, random_state=None, verbose=0, base_estimator='deprecated')


   Bases: :py:obj:`BaseBagging`, :py:obj:`sklearn.ensemble.BaggingClassifier`

   
   A bagging classifier.

   A bagging regressor is a meta-estimator that fits base classifiers
   on random subsets of the original data.

   :Parameters:

       **estimator** : object, optional
           Base estimator of the ensemble. If `None`, the base estimator
           is a :class:`~wildboar.tree.ShapeletTreeRegressor`.

       **n_estimators** : int, optional
           The number of base estimators in the ensemble.

       **max_samples** : int or float, optional
           The number of samples to draw from `X` to train each base estimator.
           
           - if `int`, then draw `max_samples` samples.
           - if `float`, then draw `max_samples * n_samples` samples.

       **bootstrap** : bool, optional
           If the samples are drawn with replacement.

       **class_weight** : dict or "balanced", optional
           Weights associated with the labels
           
           - if `dict`, weights on the form `{label: weight}`.
           - if "balanced" each class weight inversely proportional to
             the class frequency.
           - if `None`, each class has equal weight.

       **oob_score** : bool, optional
           Use out-of-bag samples to estimate generalization performance. Requires
           `bootstrap=True`.

       **warm_start** : bool, optional
           When set to `True`, reuse the solution of the previous call
           to fit and add more estimators to the ensemble, otherwise, just fit
           a whole new ensemble.

       **n_jobs** : int, optional
           The number of jobs to run in parallel. A value of `None` means
           using a single core and a value of `-1` means using all cores.
           Positive integers mean the exact number of cores.

       **random_state** : int or RandomState
           Controls the random resampling of the original dataset.
           
           - If `int`, `random_state` is the seed used by the
             random number generator.
           - If :class:`numpy.random.RandomState` instance, `random_state` is
             the random number generator.
           - If `None`, the random number generator is the
             :class:`numpy.random.RandomState` instance used by
             :func:`numpy.random`.

       **verbose** : int, optional
           Controls the output to standard error while fitting and predicting.

       **base_estimator** : object, optional
           Use `estimator` instead.
           
           .. deprecated:: 1.2
               `base_estimator` has been deprecated and will be removed in 1.4.
               Use `estimator` instead.














   ..
       !! processed by numpydoc !!
   .. py:method:: fit(x, y, sample_weight=None)

      
      Build a Bagging ensemble of estimators from the training set (X, y).


      :Parameters:

          **X** : {array-like, sparse matrix} of shape (n_samples, n_features)
              The training input samples. Sparse matrices are accepted only if
              they are supported by the base estimator.

          **y** : array-like of shape (n_samples,)
              The target values (class labels in classification, real numbers in
              regression).

          **sample_weight** : array-like of shape (n_samples,), default=None
              Sample weights. If None, then samples are equally weighted.
              Note that this is supported only if the base estimator supports
              sample weighting.

      :Returns:

          **self** : object
              Fitted estimator.













      ..
          !! processed by numpydoc !!


.. py:class:: BaggingRegressor(estimator, n_estimators=100, *, max_samples=1.0, bootstrap=True, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0, base_estimator='deprecated')


   Bases: :py:obj:`BaseBagging`, :py:obj:`sklearn.ensemble.BaggingRegressor`

   
   A bagging regressor.

   A bagging regressor is a meta-estimator that fits base classifiers
   on random subsets of the original data.

   :Parameters:

       **estimator** : object, optional
           Base estimator of the ensemble. If `None`, the base estimator
           is a :class:`~wildboar.tree.ShapeletTreeRegressor`.

       **n_estimators** : int, optional
           The number of base estimators in the ensemble.

       **max_samples** : int or float, optional
           The number of samples to draw from `X` to train each base estimator.
           
           - if `int`, then draw `max_samples` samples.
           - if `float`, then draw `max_samples * n_samples` samples.

       **bootstrap** : bool, optional
           If the samples are drawn with replacement.

       **oob_score** : bool, optional
           Use out-of-bag samples to estimate generalization performance. Requires
           `bootstrap=True`.

       **warm_start** : bool, optional
           When set to `True`, reuse the solution of the previous call
           to fit and add more estimators to the ensemble, otherwise, just fit
           a whole new ensemble.

       **n_jobs** : int, optional
           The number of jobs to run in parallel. A value of `None` means
           using a single core and a value of `-1` means using all cores.
           Positive integers mean the exact number of cores.

       **random_state** : int or RandomState, optional
           Controls the random resampling of the original dataset.
           
           - If `int`, `random_state` is the seed used by the random
             number generator.
           - If :class:`numpy.random.RandomState` instance, `random_state` is
             the random number generator.
           - If `None`, the random number generator is the
             :class:`numpy.random.RandomState` instance used by
             :func:`numpy.random`.

       **verbose** : int, optional
           Controls the output to standard error while fitting and predicting.

       **base_estimator** : object, optional
           Use `estimator` instead.
           
           .. deprecated:: 1.2
               `base_estimator` has been deprecated and will be removed in 1.4.
               Use `estimator` instead.














   ..
       !! processed by numpydoc !!
   .. py:method:: fit(x, y, sample_weight=None)

      
      Build a Bagging ensemble of estimators from the training set (X, y).


      :Parameters:

          **X** : {array-like, sparse matrix} of shape (n_samples, n_features)
              The training input samples. Sparse matrices are accepted only if
              they are supported by the base estimator.

          **y** : array-like of shape (n_samples,)
              The target values (class labels in classification, real numbers in
              regression).

          **sample_weight** : array-like of shape (n_samples,), default=None
              Sample weights. If None, then samples are equally weighted.
              Note that this is supported only if the base estimator supports
              sample weighting.

      :Returns:

          **self** : object
              Fitted estimator.













      ..
          !! processed by numpydoc !!


.. py:class:: BaseBagging(estimator=None, n_estimators=10, *, max_samples=1.0, bootstrap=True, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0, base_estimator='deprecated')


   Bases: :py:obj:`wildboar.base.BaseEstimator`, :py:obj:`sklearn.ensemble._bagging.BaseBagging`

   
   Base estimator for Wildboar ensemble estimators.
















   ..
       !! processed by numpydoc !!
   .. py:method:: fit(x, y, sample_weight=None)

      
      Build a Bagging ensemble of estimators from the training set (X, y).


      :Parameters:

          **X** : {array-like, sparse matrix} of shape (n_samples, n_features)
              The training input samples. Sparse matrices are accepted only if
              they are supported by the base estimator.

          **y** : array-like of shape (n_samples,)
              The target values (class labels in classification, real numbers in
              regression).

          **sample_weight** : array-like of shape (n_samples,), default=None
              Sample weights. If None, then samples are equally weighted.
              Note that this is supported only if the base estimator supports
              sample weighting.

      :Returns:

          **self** : object
              Fitted estimator.













      ..
          !! processed by numpydoc !!


.. py:class:: ExtraShapeletTreesClassifier(n_estimators=100, *, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, metric='euclidean', metric_params=None, criterion='entropy', oob_score=False, bootstrap=True, warm_start=False, class_weight=None, n_jobs=None, random_state=None)


   Bases: :py:obj:`BaseShapeletForestClassifier`

   
   An ensemble of extremely random shapelet trees.


   :Parameters:

       **n_estimators** : int, optional
           The number of estimators.

       **max_depth** : int, optional
           The maximum depth of the tree. If `None` the tree is expanded
           until all leaves are pure or until all leaves contain less than
           `min_samples_split` samples.

       **min_samples_split** : int, optional
           The minimum number of samples to split an internal node.

       **min_samples_leaf** : int, optional
           The minimum number of samples in a leaf.

       **min_impurity_decrease** : float, optional
           A split will be introduced only if the impurity decrease is larger than
           or equal to this value.

       **min_shapelet_size** : float, optional
           The minimum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **max_shapelet_size** : float, optional
           The maximum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **metric** : str or list, optional
           The distance metric.
           
           - If `str`, the distance metric used to identify the best
             shapelet.
           - If `list`, multiple metrics specified as a list of tuples,
             where the first element of the tuple is a metric name and the second
             element a dictionary with a parameter grid specification. A parameter
             grid specification is a dict with two mandatory and one optional
             key-value pairs defining the lower and upper bound on the values and
             number of values in the grid. For example, to specifiy a grid over
             the argument `r` with 10 values in the range 0 to 1, we would give
             the following specification: `dict(min_r=0, max_r=1, num_r=10)`.
           
             Read more about metric specifications in the `User guide
             <metric_specification>`__.
           
           .. versionchanged:: 1.2
               Added support for multi-metric shapelet transform

       **metric_params** : dict, optional
           Parameters for the distance measure. Ignored unless metric is a string.
           
           Read more about the parameters in the `User guide
           <list_of_subsequence_metrics>`__.

       **criterion** : {"entropy", "gini"}, optional
           The criterion used to evaluate the utility of a split

       **oob_score** : bool, optional
           Use out-of-bag samples to estimate generalization performance. Requires
           `bootstrap=True`.

       **bootstrap** : bool, optional
           If the samples are drawn with replacement.

       **warm_start** : bool, optional
           When set to `True`, reuse the solution of the previous call to
           fit and add more estimators to the ensemble, otherwise, just fit a
           whole new ensemble.

       **class_weight** : dict or "balanced", optional
           Weights associated with the labels
           
           - if `dict`, weights on the form `{label: weight}`
           - if "balanced" each class weight inversely proportional to
             the class frequency
           - if :class:`None`, each class has equal weight

       **n_jobs** : int, optional
           The number of jobs to run in parallel. A value of `None` means
           using a single core and a value of `-1` means using all cores.
           Positive integers mean the exact number of cores.

       **random_state** : int or RandomState
           Controls the random resampling of the original dataset.
           
           - If `int`, `random_state` is the seed used by the
             random number generator
           - If :class:`numpy.random.RandomState` instance, `random_state` is
             the random number generator
           - If `None`, the random number generator is the
             :class:`numpy.random.RandomState` instance used by
             :func:`numpy.random`.











   .. rubric:: Examples

   >>> from wildboar.ensemble import ExtraShapeletTreesClassifier
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ExtraShapeletTreesClassifier(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   ExtraShapeletTreesClassifier(metric='scaled_euclidean')
   >>> y_hat = f.predict(x)



   ..
       !! processed by numpydoc !!

.. py:class:: ExtraShapeletTreesRegressor(n_estimators=100, *, max_depth=None, min_samples_split=2, min_shapelet_size=0, max_shapelet_size=1, metric='euclidean', metric_params=None, criterion='squared_error', oob_score=False, bootstrap=True, warm_start=False, n_jobs=None, random_state=None)


   Bases: :py:obj:`BaseShapeletForestRegressor`

   
   An ensemble of extremely random shapelet tree regressors.


   :Parameters:

       **n_estimators** : int, optional
           The number of estimators.

       **max_depth** : int, optional
           The maximum depth of the tree. If `None` the tree is expanded
           until all leaves are pure or until all leaves contain less than
           `min_samples_split` samples.

       **min_samples_split** : int, optional
           The minimum number of samples to split an internal node.

       **min_shapelet_size** : float, optional
           The minimum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **max_shapelet_size** : float, optional
           The maximum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **metric** : str or list, optional
           The distance metric.
           
           - If `str`, the distance metric used to identify the best
             shapelet.
           - If `list`, multiple metrics specified as a list of tuples,
             where the first element of the tuple is a metric name and the second
             element a dictionary with a parameter grid specification. A parameter
             grid specification is a dict with two mandatory and one optional
             key-value pairs defining the lower and upper bound on the values and
             number of values in the grid. For example, to specifiy a grid over
             the argument `r` with 10 values in the range 0 to 1, we would give
             the following specification: `dict(min_r=0, max_r=1, num_r=10)`.
           
             Read more about metric specifications in the `User guide
             <metric_specification>`__.
           
           .. versionchanged:: 1.2
               Added support for multi-metric shapelet transform

       **metric_params** : dict, optional
           Parameters for the distance measure. Ignored unless metric is a string.
           
           Read more about the parameters in the `User guide
           <list_of_subsequence_metrics>`__.

       **criterion** : {"squared_error"}, optional
           The criterion used to evaluate the utility of a split.
           
           .. deprecated:: 1.1
               Criterion "mse" was deprecated in v1.1 and removed in version 1.2.

       **oob_score** : bool, optional
           Use out-of-bag samples to estimate generalization performance. Requires
           `bootstrap=True`.

       **bootstrap** : bool, optional
           If the samples are drawn with replacement.

       **warm_start** : bool, optional
           When set to `True`, reuse the solution of the previous call to
           fit and add more estimators to the ensemble, otherwise, just fit a
           whole new ensemble.

       **n_jobs** : int, optional
           The number of jobs to run in parallel. A value of `None` means
           using a single core and a value of `-1` means using all cores.
           Positive integers mean the exact number of cores.

       **random_state** : int or RandomState
           Controls the random resampling of the original dataset.
           
           - If int, `random_state` is the seed used by the
             random number generator
           - If :class:`numpy.random.RandomState` instance, `random_state` is
             the random number generator
           - If `None`, the random number generator is the
             :class:`numpy.random.RandomState` instance used by
             :func:`numpy.random`.











   .. rubric:: Examples

   >>> from wildboar.ensemble import ExtraShapeletTreesRegressor
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ExtraShapeletTreesRegressor(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   ExtraShapeletTreesRegressor(metric='scaled_euclidean')
   >>> y_hat = f.predict(x)



   ..
       !! processed by numpydoc !!

.. py:class:: IntervalForestClassifier(n_estimators=100, *, n_intervals='sqrt', intervals='fixed', summarizer='mean_var_std', sample_size=0.5, min_size=0.0, max_size=1.0, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)


   Bases: :py:obj:`BaseForestClassifier`

   
   An ensemble of interval tree classifiers.
















   ..
       !! processed by numpydoc !!

.. py:class:: IntervalForestRegressor(n_estimators=100, *, n_intervals='sqrt', intervals='fixed', summarizer='auto', sample_size=0.5, min_size=0.0, max_size=1.0, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='squared_error', bootstrap=True, warm_start=False, n_jobs=None, random_state=None)


   Bases: :py:obj:`BaseForestRegressor`

   
   An ensemble of interval tree regressors.
















   ..
       !! processed by numpydoc !!

.. py:class:: IsolationShapeletForest(n_estimators=100, *, n_shapelets=1, bootstrap=False, n_jobs=None, min_shapelet_size=0, max_shapelet_size=1, min_samples_split=2, max_samples='auto', contamination='auto', warm_start=False, metric='euclidean', metric_params=None, random_state=None)


   Bases: :py:obj:`sklearn.base.OutlierMixin`, :py:obj:`ForestMixin`, :py:obj:`BaseBagging`

   
   An isolation shapelet forest.

   .. versionadded:: 0.3.5

   :Parameters:

       **n_estimators** : int, optional
           The number of estimators in the ensemble.

       **n_shapelets** : int, optional
           The number of shapelets to sample at each node.

       **bootstrap** : bool, optional
           If the samples are drawn with replacement.

       **n_jobs** : int, optional
           The number of jobs to run in parallel. A value of `None` means using a
           single core and a value of `-1` means using all cores. Positive
           integers mean the exact number of cores.

       **min_shapelet_size** : float, optional
           The minimum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **max_shapelet_size** : float, optional
           The maximum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **min_samples_split** : int, optional
           The minimum number of samples to split an internal node.

       **max_samples** : "auto", float or int, optional
           The number of samples to draw to train each base estimator.

       **contamination** : 'auto' or float, optional
           The strategy for computing the offset.
           
           - if "auto" then `offset_` is set to `-0.5`.
           - if float `offset_` is computed as the c:th percentile of
             scores.
           
           If `bootstrap=True`, out-of-bag samples are used for computing
           the scores.

       **warm_start** : bool, optional
           When set to `True`, reuse the solution of the previous call to
           fit and add more estimators to the ensemble, otherwise, just fit a
           whole new ensemble.

       **metric** : str or list, optional
           The distance metric.
           
           - If `str`, the distance metric used to identify the best
             shapelet.
           - If `list`, multiple metrics specified as a list of tuples,
             where the first element of the tuple is a metric name and the second
             element a dictionary with a parameter grid specification. A parameter
             grid specification is a dict with two mandatory and one optional
             key-value pairs defining the lower and upper bound on the values and
             number of values in the grid. For example, to specifiy a grid over
             the argument `r` with 10 values in the range 0 to 1, we would give
             the following specification: `dict(min_r=0, max_r=1, num_r=10)`.
           
             Read more about metric specifications in the `User guide
             <metric_specification>`__.
           
           .. versionchanged:: 1.2
               Added support for multi-metric shapelet transform

       **metric_params** : dict, optional
           Parameters for the distance measure. Ignored unless metric is a string.
           
           Read more about the parameters in the `User guide
           <list_of_subsequence_metrics>`__.

       **random_state** : int or RandomState
           Controls the random resampling of the original dataset.
           
           - If `int`, `random_state` is the seed used by the
             random number generator.
           - If :class:`numpy.random.RandomState` instance, `random_state` is
             the random number generator.
           - If `None`, the random number generator is the
             :class:`numpy.random.RandomState` instance used by
             :func:`numpy.random`.











   .. rubric:: Examples

   Using default offset threshold

   >>> from wildboar.ensemble import IsolationShapeletForest
   >>> from wildboar.datasets import load_two_lead_ecg
   >>> from wildboar.model_selection import outlier_train_test_split
   >>> from sklearn.metrics import balanced_accuracy_score
   >>> f = IsolationShapeletForest(random_state=1)
   >>> x, y = load_two_lead_ecg()
   >>> x_train, x_test, y_train, y_test = outlier_train_test_split(
   ...     x, y, 1, test_size=0.2, anomalies_train_size=0.05, random_state=1
   ... )
   >>> f.fit(x_train)
   IsolationShapeletForest(random_state=1)
   >>> y_pred = f.predict(x_test)
   >>> balanced_accuracy_score(y_test, y_pred) # doctest: +NUMBER
   0.8674

   :Attributes:

       **offset_** : float
           The offset for computing the final decision


   ..
       !! processed by numpydoc !!
   .. py:method:: decision_function(x)


   .. py:method:: fit(x, y=None, sample_weight=None)

      
      Build a Bagging ensemble of estimators from the training set (X, y).


      :Parameters:

          **X** : {array-like, sparse matrix} of shape (n_samples, n_features)
              The training input samples. Sparse matrices are accepted only if
              they are supported by the base estimator.

          **y** : array-like of shape (n_samples,)
              The target values (class labels in classification, real numbers in
              regression).

          **sample_weight** : array-like of shape (n_samples,), default=None
              Sample weights. If None, then samples are equally weighted.
              Note that this is supported only if the base estimator supports
              sample weighting.

      :Returns:

          **self** : object
              Fitted estimator.













      ..
          !! processed by numpydoc !!

   .. py:method:: predict(x)


   .. py:method:: score_samples(x)



.. py:class:: PivotForestClassifier(n_estimators=100, *, n_pivot='sqrt', metrics='all', oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)


   Bases: :py:obj:`BaseForestClassifier`

   
   An ensemble of interval tree classifiers.
















   ..
       !! processed by numpydoc !!

.. py:class:: ProximityForestClassifier(n_estimators=100, *, n_pivot=1, pivot_sample='label', metric_sample='weighted', metric='auto', metric_params=None, metric_factories=None, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0, criterion='entropy', bootstrap=True, warm_start=False, n_jobs=None, class_weight=None, random_state=None)


   Bases: :py:obj:`BaseForestClassifier`

   
   A forest of proximity trees.


   :Parameters:

       **n_estimators** : int, optional
           The number of estimators.

       **n_pivot** : int, optional
           The number of pivots to sample at each node.

       **pivot_sample** : {"label", "uniform"}, optional
           The pivot sampling method.

       **metric_sample** : {"uniform", "weighted"}, optional
           The metric sampling method.

       **metric** : {"auto", "default"}, str or list, optional
           The distance metrics. By default, we use the parameterization suggested by
           Lucas et.al (2019).
           
           - If "auto", use the default metric specification, suggested by
             (Lucas et. al, 2020).
           - If str, use a single metric or default metric specification.
           - If list, custom metric specification can be given as a list of
             tuples, where the first element of the tuple is a metric name and the
             second element a dictionary with a parameter grid specification. A
             parameter grid specification is a `dict` with two mandatory and one
             optional key-value pairs defining the lower and upper bound on the
             values as well as the number of values in the grid. For example, to
             specifiy a grid over the argument 'r' with 10 values in the range 0
             to 1, we would give the following specification: 
             `dict(min_r=0, max_r=1, num_r=10)`.
           
           Read more about the metrics and their parameters in the
           :ref:`User guide <list_of_metrics>`.

       **metric_params** : dict, optional
           Parameters for the distance measure. Ignored unless metric is a string.
           
           Read more about the parameters in the :ref:`User guide
           <list_of_metrics>`.

       **metric_factories** : dict, optional
           A metric specification.
           
           .. deprecated:: 1.2
               Use the combination of metric and metric params.

       **oob_score** : bool, optional
           Use out-of-bag samples to estimate generalization performance. Requires
           `bootstrap=True`.

       **max_depth** : int, optional
           The maximum tree depth.

       **min_samples_split** : int, optional
           The minimum number of samples to consider a split.

       **min_samples_leaf** : int, optional
           The minimum number of samples in a leaf.

       **min_impurity_decrease** : float, optional
           The minimum impurity decrease to build a sub-tree.

       **criterion** : {"entropy", "gini"}, optional
           The impurity criterion.

       **bootstrap** : bool, optional
           If the samples are drawn with replacement.

       **warm_start** : bool, optional
           When set to `True`, reuse the solution of the previous call
           to fit and add more estimators to the ensemble, otherwise, just fit
           a whole new ensemble.

       **n_jobs** : int, optional
           The number of jobs to run in parallel. A value of `None` means
           using a single core and a value of `-1` means using all cores.
           Positive integers mean the exact number of cores.

       **class_weight** : dict or "balanced", optional
           Weights associated with the labels.
           
           - if dict, weights on the form {label: weight}.
           - if "balanced" each class weight inversely proportional to the class
               frequency.
           - if None, each class has equal weight.

       **random_state** : int or RandomState
           - If `int`, `random_state` is the seed used by the random number generator
           - If `RandomState` instance, `random_state` is the random number generator
           - If `None`, the random number generator is the `RandomState` instance used
               by `np.random`.










   .. rubric:: References

   Lucas, Benjamin, Ahmed Shifaz, Charlotte Pelletier, Lachlan O'Neill, Nayyar Zaidi,     Bart Goethals, François Petitjean, and Geoffrey I. Webb. (2019)
       Proximity forest: an effective and scalable distance-based classifier for time
       series. Data Mining and Knowledge Discovery

   .. only:: latex

      




   ..
       !! processed by numpydoc !!

.. py:class:: RocketForestClassifier(n_estimators=100, *, n_kernels=10, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, sampling='normal', sampling_params=None, kernel_size=None, min_size=None, max_size=None, bias_prob=1.0, normalize_prob=1.0, padding_prob=0.5, criterion='entropy', bootstrap=True, warm_start=False, class_weight=None, n_jobs=None, random_state=None)


   Bases: :py:obj:`BaseForestClassifier`

   
   An ensemble of rocket tree classifiers.
















   ..
       !! processed by numpydoc !!

.. py:class:: RocketForestRegressor(n_estimators=100, *, n_kernels=10, oob_score=False, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, sampling='normal', sampling_params=None, kernel_size=None, min_size=None, max_size=None, bias_prob=1.0, normalize_prob=1.0, padding_prob=0.5, criterion='squared_error', bootstrap=True, warm_start=False, n_jobs=None, random_state=None)


   Bases: :py:obj:`BaseForestRegressor`

   
   An ensemble of rocket tree regressors.


   :Parameters:

       **n_estimators** : int, optional
           The number of estimators.

       **n_kernels** : int, optional
           The number of shapelets to sample at each node.

       **oob_score** : bool, optional
           Use out-of-bag samples to estimate generalization performance. Requires
           `bootstrap=True`.

       **max_depth** : int, optional
           The maximum depth of the tree. If `None` the tree is
           expanded until all leaves are pure or until all leaves contain less
           than `min_samples_split` samples.

       **min_samples_split** : int, optional
           The minimum number of samples to split an internal node.

       **min_samples_leaf** : int, optional
           The minimum number of samples in a leaf.

       **min_impurity_decrease** : float, optional
           A split will be introduced only if the impurity decrease is larger
           than or equal to this value.

       **sampling** : {"normal", "uniform", "shapelet"}, optional
           The sampling of convolutional filters.
           
           - if "normal", sample filter according to a normal distribution with
               ``mean`` and ``scale``.
           - if "uniform", sample filter according to a uniform distribution with
               ``lower`` and ``upper``.
           - if "shapelet", sample filters as subsequences in the training data.

       **sampling_params** : dict, optional
           The parameters for the sampling.
           
           - if "normal", ``{"mean": float, "scale": float}``, defaults to
               ``{"mean": 0, "scale": 1}``.
           - if "uniform", ``{"lower": float, "upper": float}``, defaults to
               ``{"lower": -1, "upper": 1}``.

       **kernel_size** : array-like, optional
           The kernel size, by default ``[7, 11, 13]``.

       **min_size** : float, optional
           The minimum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **max_size** : float, optional
           The maximum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **bias_prob** : float, optional
           The probability of using a bias term.

       **normalize_prob** : float, optional
           The probability of performing normalization.

       **padding_prob** : float, optional
           The probability of padding with zeros.

       **criterion** : {"squared_error"}, optional
           The criterion used to evaluate the utility of a split.
           
           .. deprecated:: 1.1
               Criterion "mse" was deprecated in v1.1 and removed in version 1.2.

       **bootstrap** : bool, optional
           If the samples are drawn with replacement.

       **warm_start** : bool, optional
           When set to `True`, reuse the solution of the previous call to
           fit and add more estimators to the ensemble, otherwise, just fit a
           whole new ensemble.

       **n_jobs** : int, optional
           The number of processor cores used for fitting the ensemble.

       **random_state** : int or RandomState
           Controls the random resampling of the original dataset.
           
           - If `int`, `random_state` is the seed used by the
             random number generator
           - If :class:`numpy.random.RandomState` instance, `random_state` is
             the random number generator
           - If `None`, the random number generator is the
             :class:`numpy.random.RandomState` instance used by
             :func:`numpy.random`.














   ..
       !! processed by numpydoc !!

.. py:class:: ShapeletForestClassifier(n_estimators=100, *, n_shapelets='log2', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, alpha=None, metric='euclidean', metric_params=None, criterion='entropy', oob_score=False, bootstrap=True, warm_start=False, class_weight=None, n_jobs=None, random_state=None)


   Bases: :py:obj:`BaseShapeletForestClassifier`

   
   An ensemble of random shapelet tree classifiers.

   A forest of randomized shapelet trees.

   :Parameters:

       **n_estimators** : int, optional
           The number of estimators.

       **n_shapelets** : int, optional
           The number of shapelets to sample at each node.

       **max_depth** : int, optional
           The maximum depth of the tree. If `None` the tree is
           expanded until all leaves are pure or until all leaves contain less
           than `min_samples_split` samples.

       **min_samples_split** : int, optional
           The minimum number of samples to split an internal node.

       **min_samples_leaf** : int, optional
           The minimum number of samples in a leaf.

       **min_impurity_decrease** : float, optional
           A split will be introduced only if the impurity decrease is larger
           than or equal to this value.

       **min_shapelet_size** : float, optional
           The minimum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **max_shapelet_size** : float, optional
           The maximum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **alpha** : float, optional
           Dynamically decrease the number of sampled shapelets at each node
           according to the current depth, i.e.:
           
           ::
               w = 1 - exp(-abs(alpha) * depth)
           
           - if `alpha < 0`, the number of sampled shapelets decrease from
             `n_shapelets` towards 1 with increased depth.
           - if `alpha > 0`, the number of sampled shapelets increase from `1`
             towards `n_shapelets` with increased depth.
           - if `None`, the number of sampled shapelets are the same
             independeth of depth.

       **metric** : str or list, optional
           The distance metric.
           
           - If `str`, the distance metric used to identify the best
             shapelet.
           - If `list`, multiple metrics specified as a list of tuples,
             where the first element of the tuple is a metric name and the second
             element a dictionary with a parameter grid specification. A parameter
             grid specification is a dict with two mandatory and one optional
             key-value pairs defining the lower and upper bound on the values and
             number of values in the grid. For example, to specifiy a grid over
             the argument `r` with 10 values in the range 0 to 1, we would give
             the following specification: `dict(min_r=0, max_r=1, num_r=10)`.
           
             Read more about metric specifications in the `User guide
             <metric_specification>`__.
           
           .. versionchanged:: 1.2
               Added support for multi-metric shapelet transform

       **metric_params** : dict, optional
           Parameters for the distance measure. Ignored unless metric is a string.
           
           Read more about the parameters in the `User guide
           <list_of_subsequence_metrics>`__.

       **criterion** : {"entropy", "gini"}, optional
           The criterion used to evaluate the utility of a split

       **oob_score** : bool, optional
           Use out-of-bag samples to estimate generalization performance. Requires
           `bootstrap=True`.

       **bootstrap** : bool, optional
           If the samples are drawn with replacement.

       **warm_start** : bool, optional
           When set to `True`, reuse the solution of the previous call to
           fit and add more estimators to the ensemble, otherwise, just fit a
           whole new ensemble.

       **class_weight** : dict or "balanced", optional
           Weights associated with the labels
           
           - if `dict`, weights on the form `{label: weight}`.
           - if "balanced" each class weight inversely proportional to
             the class frequency.
           - if `None`, each class has equal weight.

       **n_jobs** : int, optional
           The number of jobs to run in parallel. A value of `None` means
           using a single core and a value of `-1` means using all cores.
           Positive integers mean the exact number of cores.

       **random_state** : int or RandomState
           Controls the random resampling of the original dataset.
           
           - If `int`, `random_state` is the seed used by the
             random number generator
           - If :class:`numpy.random.RandomState` instance, `random_state` is
             the random number generator
           - If `None`, the random number generator is the
             :class:`numpy.random.RandomState` instance used by
             :func:`numpy.random`.











   .. rubric:: Examples

   >>> from wildboar.ensemble import ShapeletForestClassifier
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ShapeletForestClassifier(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   ShapeletForestClassifier(metric='scaled_euclidean')
   >>> y_hat = f.predict(x)



   ..
       !! processed by numpydoc !!

.. py:class:: ShapeletForestEmbedding(n_estimators=100, *, n_shapelets=1, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, metric='euclidean', metric_params=None, criterion='squared_error', bootstrap=True, warm_start=False, n_jobs=None, sparse_output=True, random_state=None)


   Bases: :py:obj:`BaseShapeletForestRegressor`

   
   An ensemble of random shapelet trees.

   An unsupervised transformation of a time series dataset to a
   high-dimensional sparse representation. A time series i indexed by the leaf
   that it falls into. This leads to a binary coding of a time series with as
   many ones as trees in the forest.

   The dimensionality of the resulting representation is `<= n_estimators *
   2^max_depth`

   :Parameters:

       **n_estimators** : int, optional
           The number of estimators.

       **n_shapelets** : int, optional
           The number of shapelets to sample at each node.

       **max_depth** : int, optional
           The maximum depth of the tree. If `None` the tree is expanded
           until all leaves are pure or until all leaves contain less than
           `min_samples_split` samples.

       **min_samples_split** : int, optional
           The minimum number of samples to split an internal node.

       **min_samples_leaf** : int, optional
           The minimum number of samples in a leaf.

       **min_impurity_decrease** : float, optional
           A split will be introduced only if the impurity decrease is larger than
           or equal to this value.

       **min_shapelet_size** : float, optional
           The minimum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **max_shapelet_size** : float, optional
           The maximum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **metric** : str or list, optional
           The distance metric.
           
           - If `str`, the distance metric used to identify the best
             shapelet.
           - If `list`, multiple metrics specified as a list of tuples,
             where the first element of the tuple is a metric name and the second
             element a dictionary with a parameter grid specification. A parameter
             grid specification is a dict with two mandatory and one optional
             key-value pairs defining the lower and upper bound on the values and
             number of values in the grid. For example, to specifiy a grid over
             the argument `r` with 10 values in the range 0 to 1, we would give
             the following specification: `dict(min_r=0, max_r=1, num_r=10)`.
           
             Read more about metric specifications in the `User guide
             <metric_specification>`__.
           
           .. versionchanged:: 1.2
               Added support for multi-metric shapelet transform

       **metric_params** : dict, optional
           Parameters for the distance measure. Ignored unless metric is a string.
           
           Read more about the parameters in the `User guide
           <list_of_subsequence_metrics>`__.

       **criterion** : {"squared_error"}, optional
           The criterion used to evaluate the utility of a split.
           
           .. deprecated:: 1.1
               Criterion "mse" was deprecated in v1.1 and removed in version 1.2.

       **bootstrap** : bool, optional
           If the samples are drawn with replacement.

       **warm_start** : bool, optional
           When set to `True`, reuse the solution of the previous call to
           fit and add more estimators to the ensemble, otherwise, just fit a
           whole new ensemble.

       **n_jobs** : int, optional
           The number of jobs to run in parallel. A value of `None` means
           using a single core and a value of `-1` means using all cores.
           Positive integers mean the exact number of cores.

       **sparse_output** : bool, optional
           Return a sparse CSR-matrix.

       **random_state** : int or RandomState
           Controls the random resampling of the original dataset.
           
           - If `int`, `random_state` is the seed used by the
             random number generator.
           - If :class:`numpy.random.RandomState` instance, `random_state` is
             the random number generator.
           - If `None`, the random number generator is the
             :class:`numpy.random.RandomState` instance used by
             :func:`numpy.random`.














   ..
       !! processed by numpydoc !!
   .. py:method:: fit(x, y=None, sample_weight=None)

      
      Build a Bagging ensemble of estimators from the training set (X, y).


      :Parameters:

          **X** : {array-like, sparse matrix} of shape (n_samples, n_features)
              The training input samples. Sparse matrices are accepted only if
              they are supported by the base estimator.

          **y** : array-like of shape (n_samples,)
              The target values (class labels in classification, real numbers in
              regression).

          **sample_weight** : array-like of shape (n_samples,), default=None
              Sample weights. If None, then samples are equally weighted.
              Note that this is supported only if the base estimator supports
              sample weighting.

      :Returns:

          **self** : object
              Fitted estimator.













      ..
          !! processed by numpydoc !!

   .. py:method:: fit_transform(x, y=None, sample_weight=None)


   .. py:method:: transform(x)



.. py:class:: ShapeletForestRegressor(n_estimators=100, *, n_shapelets='log2', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, min_shapelet_size=0.0, max_shapelet_size=1.0, alpha=None, metric='euclidean', metric_params=None, criterion='squared_error', oob_score=False, bootstrap=True, warm_start=False, n_jobs=None, random_state=None)


   Bases: :py:obj:`BaseShapeletForestRegressor`

   
   An ensemble of random shapelet tree regressors.


   :Parameters:

       **n_estimators** : int, optional
           The number of estimators.

       **n_shapelets** : int, optional
           The number of shapelets to sample at each node.

       **max_depth** : int, optional
           The maximum depth of the tree. If `None` the tree is
           expanded until all leaves are pure or until all leaves contain less
           than `min_samples_split` samples.

       **min_samples_split** : int, optional
           The minimum number of samples to split an internal node.

       **min_samples_leaf** : int, optional
           The minimum number of samples in a leaf.

       **min_impurity_decrease** : float, optional
           A split will be introduced only if the impurity decrease is larger
           than or equal to this value.

       **min_shapelet_size** : float, optional
           The minimum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **max_shapelet_size** : float, optional
           The maximum length of a shapelets expressed as a fraction of
           *n_timestep*.

       **alpha** : float, optional
           Dynamically decrease the number of sampled shapelets at each node
           according to the current depth, i.e.:
           
           ::
               w = 1 - exp(-abs(alpha) * depth)
           
           - if `alpha < 0`, the number of sampled shapelets decrease from
             `n_shapelets` towards 1 with increased depth.
           - if `alpha > 0`, the number of sampled shapelets increase from `1`
             towards `n_shapelets` with increased depth.
           - if `None`, the number of sampled shapelets are the same
             independeth of depth.

       **metric** : str or list, optional
           The distance metric.
           
           - If `str`, the distance metric used to identify the best
             shapelet.
           - If `list`, multiple metrics specified as a list of tuples,
             where the first element of the tuple is a metric name and the second
             element a dictionary with a parameter grid specification. A parameter
             grid specification is a dict with two mandatory and one optional
             key-value pairs defining the lower and upper bound on the values and
             number of values in the grid. For example, to specifiy a grid over
             the argument `r` with 10 values in the range 0 to 1, we would give
             the following specification: `dict(min_r=0, max_r=1, num_r=10)`.
           
             Read more about metric specifications in the `User guide
             <metric_specification>`__.
           
           .. versionchanged:: 1.2
               Added support for multi-metric shapelet transform

       **metric_params** : dict, optional
           Parameters for the distance measure. Ignored unless metric is a string.
           
           Read more about the parameters in the `User guide
           <list_of_subsequence_metrics>`__.

       **criterion** : {"squared_error"}, optional
           The criterion used to evaluate the utility of a split.
           
           .. deprecated:: 1.1
               Criterion "mse" was deprecated in v1.1 and removed in version 1.2.

       **oob_score** : bool, optional
           Use out-of-bag samples to estimate generalization performance. Requires
           `bootstrap=True`.

       **bootstrap** : bool, optional
           If the samples are drawn with replacement.

       **warm_start** : bool, optional
           When set to `True`, reuse the solution of the previous call to
           fit and add more estimators to the ensemble, otherwise, just fit a
           whole new ensemble.

       **n_jobs** : int, optional
           The number of processor cores used for fitting the ensemble.

       **random_state** : int or RandomState
           Controls the random resampling of the original dataset.
           
           - If `int`, `random_state` is the seed used by the
             random number generator
           - If :class:`numpy.random.RandomState` instance, `random_state` is
             the random number generator
           - If `None`, the random number generator is the
             :class:`numpy.random.RandomState` instance used by
             :func:`numpy.random`.











   .. rubric:: Examples

   >>> from wildboar.ensemble import ShapeletForestRegressor
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ShapeletForestRegressor(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   ShapeletForestRegressor(metric='scaled_euclidean')
   >>> y_hat = f.predict(x)



   ..
       !! processed by numpydoc !!

