:py:mod:`wildboar.ensemble`
===========================

.. py:module:: wildboar.ensemble


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   wildboar.ensemble.ExtraShapeletTreesClassifier
   wildboar.ensemble.ExtraShapeletTreesRegressor
   wildboar.ensemble.IsolationShapeletForest
   wildboar.ensemble.ShapeletForestClassifier
   wildboar.ensemble.ShapeletForestEmbedding
   wildboar.ensemble.ShapeletForestRegressor




.. py:class:: ExtraShapeletTreesClassifier(*, n_estimators=100, max_depth=None, min_samples_split=2, min_shapelet_size=0, max_shapelet_size=1, metric='euclidean', metric_params=None, oob_score=False, bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestClassifier`

   An ensemble of extremely random shapelet trees for time series regression.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ExtraShapeletTreesClassifier
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ExtraShapeletTreesClassifier(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Construct a extra shapelet trees classifier.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param random_state: Controls the random resampling of the original dataset and the construction of
                        the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional


.. py:class:: ExtraShapeletTreesRegressor(*, n_estimators=100, max_depth=None, min_samples_split=2, min_shapelet_size=0, max_shapelet_size=1, metric='euclidean', metric_params=None, oob_score=False, bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestRegressor`

   An ensemble of extremely random shapelet trees for time series regression.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ExtraShapeletTreesRegressor
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ExtraShapeletTreesRegressor(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Construct a extra shapelet trees regressor.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param random_state: Controls the random resampling of the original dataset and the construction of
                        the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional


.. py:class:: IsolationShapeletForest(*, n_estimators=100, bootstrap=False, n_jobs=None, min_shapelet_size=0, max_shapelet_size=1, min_samples_split=2, max_samples='auto', contamination='auto', contamination_set='training', warm_start=False, metric='euclidean', metric_params=None, random_state=None)

   Bases: :py:obj:`ShapeletForestMixin`, :py:obj:`sklearn.base.OutlierMixin`, :py:obj:`sklearn.ensemble._bagging.BaseBagging`

   A isolation shapelet forest.

   .. versionadded:: 0.3.5

   .. attribute:: offset_

      The offset for computing the final decision

      :type: float

   .. rubric:: Examples

   >>> from wildboar.ensemble import IsolationShapeletForest
   >>> from wildboar.datasets import load_two_lead_ecg
   >>> from model_selection.outlier import train_test_split
   >>> from sklearn.metrics import balanced_accuracy_score
   >>> x, y = load_two_lead_ecg("two_lead_ecg")
   >>> x_train, x_test, y_train, y_test = train_test_split(x, y, 1, test_size=0.2, anomalies_train_size=0.05)
   >>> f = IsolationShapeletForest(n_estimators=100, contamination=balanced_accuracy_score)
   >>> f.fit(x_train, y_train)
   >>> y_pred = f.predict(x_test)
   >>> balanced_accuracy_score(y_test, y_pred)

   Or using default offset threshold

   >>> from wildboar.ensemble import IsolationShapeletForest
   >>> from wildboar.datasets import load_two_lead_ecg
   >>> from model_selection.outlier import train_test_split
   >>> from sklearn.metrics import balanced_accuracy_score
   >>> f = IsolationShapeletForest()
   >>> x, y = load_two_lead_ecg("two_lead_ecg")
   >>> x_train, x_test, y_train, y_test = train_test_split(x, y, 1, test_size=0.2, anomalies_train_size=0.05)
   >>> f.fit(x_train)
   >>> y_pred = f.predict(x_test)
   >>> balanced_accuracy_score(y_test, y_pred)

   Construct a shapelet isolation forest

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param max_samples: The number of samples to draw to train each base estimator
   :type max_samples: float or int
   :param contamination: The strategy for computing the offset (see `offset_`)

                         - if 'auto' ``offset_=-0.5``
                         - if 'auc' ``offset_`` is computed as the offset that maximizes the
                           area under ROC in the training or out-of-bag set (see ``contamination_set``).
                         - if 'prc' ``offset_`` is computed as the offset that maximizes the
                           area under PRC in the training or out-of-bag set (see ``contamination_set``)
                         - if callable ``offset_`` is computed as the offset that maximizes the score
                           computed by the callable in training or out-of-bag set (see ``contamination_set``)
                         - if float ``offset_`` is computed as the c:th percentile of scores in the training
                           or out-of-bag set (see ``contamination_set``)

                         Setting contamination to either 'auc' or 'prc' require that `y` is passed to `fit`.
   :type contamination: str, float or callable
   :param contamination_set: Compute the ``offset_`` from either the out-of-bag samples or the training samples.
                             'oob' require `bootstrap=True`.
   :type contamination_set: {'training', 'oob'}, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param random_state: Controls the random resampling of the original dataset and the construction of
                        the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional

   .. py:method:: decision_function(x)


   .. py:method:: fit(x, y=None, sample_weight=None, check_input=True)

      Build a Bagging ensemble of estimators from the training set (X, y).

      :param X: The training input samples. Sparse matrices are accepted only if
                they are supported by the base estimator.
      :type X: {array-like, sparse matrix} of shape (n_samples, n_features)
      :param y: The target values (class labels in classification, real numbers in
                regression).
      :type y: array-like of shape (n_samples,)
      :param sample_weight: Sample weights. If None, then samples are equally weighted.
                            Note that this is supported only if the base estimator supports
                            sample weighting.
      :type sample_weight: array-like of shape (n_samples,), default=None

      :returns: **self** -- Fitted estimator.
      :rtype: object


   .. py:method:: predict(x)


   .. py:method:: score_samples(x)



.. py:class:: ShapeletForestClassifier(*, n_estimators=100, n_shapelets=10, max_depth=None, min_samples_split=2, min_shapelet_size=0, max_shapelet_size=1, metric='euclidean', metric_params=None, oob_score=False, bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestClassifier`

   An ensemble of random shapelet tree classifiers.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ShapeletForestClassifier
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ShapeletForestClassifier(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Shapelet forest classifier.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param n_shapelets: The number of shapelets to sample at each node
   :type n_shapelets: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param oob_score: Compute out-of-bag estimates of the ensembles performance.
   :type oob_score: bool, optional
   :param random_state: Controls the random resampling of the original dataset and the construction of
                        the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional


.. py:class:: ShapeletForestEmbedding(n_estimators=100, *, n_shapelets=1, max_depth=5, min_samples_split=2, min_shapelet_size=0, max_shapelet_size=1, metric='euclidean', metric_params=None, bootstrap=True, warm_start=False, n_jobs=None, sparse_output=True, random_state=None)

   Bases: :py:obj:`BaseShapeletForestRegressor`

   An ensemble of random shapelet trees

   An unsupervised transformation of a time series dataset
   to a high-dimensional sparse representation. A time series i
   indexed by the leaf that it falls into. This leads to a binary
   coding of a time series with as many ones as trees in the forest.

   The dimensionality of the resulting representation is
   ``<= n_estimators * 2^max_depth``

   Construct a shapelet forest embedding.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param sparse_output: Return a sparse CSR-matrix.
   :type sparse_output: bool, optional
   :param random_state: Controls the random resampling of the original dataset and the construction of
                        the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional

   .. py:method:: fit(x, y=None, sample_weight=None, check_input=True)

      Fit a random shapelet forest regressor


   .. py:method:: fit_transform(x, y=None, sample_weight=None, check_input=True)


   .. py:method:: transform(x)



.. py:class:: ShapeletForestRegressor(*, n_estimators=100, n_shapelets=10, max_depth=None, min_samples_split=2, min_shapelet_size=0, max_shapelet_size=1, metric='euclidean', metric_params=None, oob_score=False, bootstrap=True, warm_start=False, n_jobs=None, random_state=None)

   Bases: :py:obj:`BaseShapeletForestRegressor`

   An ensemble of random shapelet regression trees.

   .. rubric:: Examples

   >>> from wildboar.ensemble import ShapeletForestRegressor
   >>> from wildboar.datasets import load_synthetic_control
   >>> x, y = load_synthetic_control()
   >>> f = ShapeletForestRegressor(n_estimators=100, metric='scaled_euclidean')
   >>> f.fit(x, y)
   >>> y_hat = f.predict(x)

   Shapelet forest regressor.

   :param n_estimators: The number of estimators
   :type n_estimators: int, optional
   :param n_shapelets: The number of shapelets to sample at each node
   :type n_shapelets: int, optional
   :param bootstrap: Use bootstrap sampling to fit the base estimators
   :type bootstrap: bool, optional
   :param n_jobs: The number of processor cores used for fitting the ensemble
   :type n_jobs: int, optional
   :param min_shapelet_size: The minimum shapelet size to sample
   :type min_shapelet_size: float, optional
   :param max_shapelet_size: The maximum shapelet size to sample
   :type max_shapelet_size: float, optional
   :param min_samples_split: The minimum samples required to split the decision trees
   :type min_samples_split: int, optional
   :param warm_start: When set to True, reuse the solution of the previous call to fit
                      and add more estimators to the ensemble, otherwise, just fit
                      a whole new ensemble.
   :type warm_start: bool, optional
   :param metric: Set the metric used to compute the distance between shapelet and time series
   :type metric: {'euclidean', 'scaled_euclidean', 'scaled_dtw'}, optional
   :param metric_params: Parameters passed to the metric construction
   :type metric_params: dict, optional
   :param oob_score: Compute out-of-bag estimates of the ensembles performance.
   :type oob_score: bool, optional
   :param random_state: Controls the random resampling of the original dataset and the construction of
                        the base estimators. Pass an int for reproducible output across multiple
                        function calls.
   :type random_state: int or RandomState, optional


